{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12lFXQTVE_F6g_RmplPlnxwAxBRE4twO1",
      "authorship_tag": "ABX9TyNWaJW/62Gti9xO9r8TysuF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhSxPRJXneyD"
      },
      "outputs": [],
      "source": [
        "pip install torch wandb pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import wandb\n",
        "import editdistance\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "LANG = 'te'\n",
        "data_path = f'/content/drive/MyDrive/dakshina_dataset_v1.0/{LANG}/lexicons/'\n",
        "\n",
        "def read_data(filepath, max_len=40):\n",
        "    pairs = []\n",
        "    # Open the file with UTF-8 encoding to properly read Unicode characters\n",
        "    with open(filepath, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            # Remove leading/trailing whitespace and split by tab\n",
        "            parts = line.strip().split('\\t')\n",
        "            # Skip lines that don't contain both source and target text\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            devanagiri, latin = parts[0], parts[1]\n",
        "\n",
        "            # We are training a Latin â†’ Devanagiri transliteration model,\n",
        "            # so set Latin as the source and Devanagiri as the target\n",
        "            source, target = latin, devanagiri\n",
        "\n",
        "            # Only keep pairs where both source and target are within the allowed max length\n",
        "            if len(source) <= max_len and len(target) <= max_len:\n",
        "                pairs.append((source, target))\n",
        "\n",
        "    # Return the list of filtered (source, target) pairs\n",
        "    return pairs\n",
        "\n",
        "def make_vocab(sequences):\n",
        "    # Initialize the vocabulary with special tokens\n",
        "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n",
        "    idx = 3  # Starting index for regular characters\n",
        "\n",
        "    # Loop through all sequences to build the vocabulary\n",
        "    for seq in sequences:\n",
        "        for ch in seq:\n",
        "            # Add each unique character to the vocabulary\n",
        "            if ch not in vocab:\n",
        "                vocab[ch] = idx\n",
        "                idx += 1\n",
        "\n",
        "    # Create reverse mapping from index to character\n",
        "    idx2char = {i: c for c, i in vocab.items()}\n",
        "\n",
        "    # Return both the character-to-index and index-to-character dictionaries\n",
        "    return vocab, idx2char\n",
        "\n",
        "def encode_word(word, vocab):\n",
        "    # Convert a word into a list of indices using the vocabulary\n",
        "    # Add <sos> token at the beginning and <eos> token at the end\n",
        "    return [vocab['<sos>']] + [vocab[ch] for ch in word] + [vocab['<eos>']]\n",
        "\n",
        "def pad_seq(seq, max_len, pad_idx=0):\n",
        "    # Pad the sequence with <pad> tokens (default index 0) to reach max_len\n",
        "    return seq + [pad_idx] * (max_len - len(seq))\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, pairs, source_vocab, target_vocab):\n",
        "        # Save padding indices for both source and target vocabularies\n",
        "        self.source_pad = source_vocab['<pad>']\n",
        "        self.target_pad = target_vocab['<pad>']\n",
        "        self.data = []\n",
        "\n",
        "        # Convert each (source, target) word pair into sequences of token indices\n",
        "        for source, target in pairs:\n",
        "            source_t = encode_word(source, source_vocab)\n",
        "            target_t = encode_word(target, target_vocab)\n",
        "            self.data.append((source_t, target_t))\n",
        "\n",
        "        # Determine the maximum lengths of source and target sequences\n",
        "        self.source_max = max(len(x[0]) for x in self.data)\n",
        "        self.target_max = max(len(x[1]) for x in self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return total number of samples in the dataset\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch a source-target pair and pad both to their respective max lengths\n",
        "        source, target = self.data[idx]\n",
        "        source = pad_seq(source, self.source_max, self.source_pad)\n",
        "        target = pad_seq(target, self.target_max, self.target_pad)\n",
        "        return torch.tensor(source), torch.tensor(target)\n",
        "\n",
        "class translit_Encoder(nn.Module):\n",
        "    def __init__(self, input_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell='lstm'):\n",
        "        super().__init__()\n",
        "        # Converts token indices into dense embeddings\n",
        "        self.embedding = nn.Embedding(input_dimensions, emb_dimensions)\n",
        "\n",
        "        # Choose the appropriate RNN variant (RNN, GRU, or LSTM)\n",
        "        rnn_cls = {'rnn': nn.RNN, 'gru': nn.GRU, 'lstm': nn.LSTM}[cell.lower()]\n",
        "        self.rnn = rnn_cls(\n",
        "            emb_dimensions,\n",
        "            hid_dimensions,\n",
        "            num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.cell = cell.lower()\n",
        "\n",
        "    def forward(self, source):\n",
        "        # Embed the input sequence\n",
        "        embedded = self.embedding(source)\n",
        "\n",
        "        # Forward pass through the RNN\n",
        "        if self.cell == 'lstm':\n",
        "            outputs, (hidden, cell) = self.rnn(embedded)\n",
        "            return hidden, cell\n",
        "        else:\n",
        "            outputs, hidden = self.rnn(embedded)\n",
        "            return hidden, None\n",
        "\n",
        "class translit_Decoder(nn.Module):\n",
        "    def __init__(self, output_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell='lstm'):\n",
        "        super().__init__()\n",
        "        # Embedding for decoder inputs\n",
        "        self.embedding = nn.Embedding(output_dimensions, emb_dimensions)\n",
        "\n",
        "        # Choose RNN type for the decoder\n",
        "        rnn_cls = {'rnn': nn.RNN, 'gru': nn.GRU, 'lstm': nn.LSTM}[cell.lower()]\n",
        "        self.rnn = rnn_cls(\n",
        "            emb_dimensions,\n",
        "            hid_dimensions,\n",
        "            num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Fully connected layer to project hidden states to vocabulary size\n",
        "        self.fc_out = nn.Linear(hid_dimensions, output_dimensions)\n",
        "\n",
        "        self.cell = cell.lower()\n",
        "\n",
        "    def forward(self, input, hidden, cell=None):\n",
        "        # Add time-step dimension (batch_size -> batch_size x 1)\n",
        "        input = input.unsqueeze(1)\n",
        "\n",
        "        # Embed input token\n",
        "        embedded = self.embedding(input)\n",
        "\n",
        "        # Forward pass through the RNN cell\n",
        "        if self.cell == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        else:\n",
        "            output, hidden = self.rnn(embedded, hidden)\n",
        "            cell = None\n",
        "\n",
        "        # Convert final hidden state to vocabulary prediction\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        # Linear layer to compute attention scores from hidden states\n",
        "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask=None):\n",
        "        # Repeat decoder hidden state across source sequence length\n",
        "        src_len = encoder_outputs.size(1)\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        # Concatenate hidden state and encoder outputs, then pass through attention layers\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "\n",
        "        # Optionally apply mask to ignore padding tokens\n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        # Normalize attention scores\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "class translit_Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.size(0)\n",
        "        target_len = target.size(1)\n",
        "        output_dimensions = self.decoder.fc_out.out_features\n",
        "\n",
        "        # Initialize tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, target_len, output_dimensions).to(self.device)\n",
        "\n",
        "        # Encode the input sequence\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        # Start decoding with the <sos> token\n",
        "        input = target[:, 0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Get output prediction and next hidden state\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            # Decide whether to use teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            # Use actual next token (teacher forcing) or predicted token\n",
        "            input = target[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "def strip_after_eos(seq, eos_idx):\n",
        "    # Convert tensor to list if needed\n",
        "    if isinstance(seq, torch.Tensor):\n",
        "        seq = seq.cpu().numpy().tolist()\n",
        "    # Trim the sequence at the first <eos> token\n",
        "    if eos_idx in seq:\n",
        "        return seq[:seq.index(eos_idx)]\n",
        "    return seq\n",
        "\n",
        "def calculate_word_accuracy(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    correct = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        # Remove padding and stop at <eos> for fair comparison\n",
        "        pred = strip_after_eos(pred, eos_idx) if eos_idx else pred\n",
        "        target = strip_after_eos(target, eos_idx) if eos_idx else target\n",
        "        pred = [p for p in pred if p != pad_idx]\n",
        "        target = [t for t in target if t != pad_idx]\n",
        "        # Count if full predicted word matches target\n",
        "        correct += int(pred == target)\n",
        "    return correct / max(len(preds), 1)\n",
        "\n",
        "\n",
        "def calculate_cer(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    cer = 0\n",
        "    total = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        # Clean sequences by removing padding and trimming after <eos>\n",
        "        pred = strip_after_eos(pred, eos_idx) if eos_idx else pred\n",
        "        target = strip_after_eos(target, eos_idx) if eos_idx else target\n",
        "        pred = [p for p in pred if p != pad_idx]\n",
        "        target = [t for t in target if t != pad_idx]\n",
        "        # Accumulate edit distance and total characters\n",
        "        cer += editdistance.eval(pred, target)\n",
        "        total += max(len(target), 1)\n",
        "    return cer / total if total > 0 else float('inf')\n",
        "\n",
        "\n",
        "def calculate_accuracy(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        # Convert tensors to lists if necessary\n",
        "        if isinstance(pred, torch.Tensor):\n",
        "            pred = pred.cpu().tolist()\n",
        "        if isinstance(target, torch.Tensor):\n",
        "            target = target.cpu().tolist()\n",
        "        # Strip <eos> tokens if specified\n",
        "        if eos_idx is not None:\n",
        "            pred = strip_after_eos(pred, eos_idx)\n",
        "            target = strip_after_eos(target, eos_idx)\n",
        "        # Compare tokens one by one, ignoring padding\n",
        "        for p_token, t_token in zip(pred, target):\n",
        "            if t_token == pad_idx:\n",
        "                continue\n",
        "            if p_token == t_token:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return correct / total if total > 0 else 0.0"
      ],
      "metadata": {
        "id": "Sqry1mukh_mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "0EvudYf1GXOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(project=\"dakshina-seq2seq-3\", entity=\"sai-sakunthala-indian-institute-of-technology-madras\", name=\"evaluate_test\")\n",
        "artifact = run.use_artifact('best_model:v5', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "# Read data and create vocabularies\n",
        "test_pairs = read_data(data_path + f\"{LANG}.translit.sampled.test.tsv\", max_len=30)\n",
        "train_pairs = read_data(data_path + f\"{LANG}.translit.sampled.train.tsv\", max_len=30)\n",
        "source_vocab, idx2char_src = make_vocab([x[0] for x in train_pairs])\n",
        "target_vocab, idx2char_tgt = make_vocab([x[1] for x in train_pairs])\n",
        "\n",
        "# Initialize model\n",
        "encoder = translit_Encoder(len(source_vocab), 128, 128*2, 2, 0.2, 'lstm').to(device)\n",
        "decoder = translit_Decoder(len(target_vocab), 128, 128*2, 2, 0.2, 'lstm').to(device)\n",
        "model = translit_Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "# Load model weights\n",
        "state_dict = torch.load(f\"{artifact_dir}/best_model.pt\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Create test dataset and loader\n",
        "test_translit = TransliterationDataset(test_pairs, source_vocab, target_vocab)\n",
        "test_loader = DataLoader(test_translit, batch_size=64, shuffle=False, drop_last=True)\n",
        "\n",
        "all_src, all_preds, all_tgts = [], [], []\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "def predict(model, src, max_len=30):\n",
        "    \"\"\"Greedy decoding implementation\"\"\"\n",
        "    encoder_hidden, encoder_cell = model.encoder(src)\n",
        "\n",
        "    # First input is SOS token\n",
        "    input = torch.tensor([target_vocab['<sos>']] * src.size(0)).to(device)\n",
        "    outputs = []\n",
        "\n",
        "    for t in range(max_len):\n",
        "        output, encoder_hidden, encoder_cell = model.decoder(input, encoder_hidden, encoder_cell)\n",
        "        input = output.argmax(1)\n",
        "        outputs.append(input)\n",
        "\n",
        "        # Stop if all sequences predicted EOS\n",
        "        if (input == target_vocab['<eos>']).all():\n",
        "            break\n",
        "\n",
        "    return torch.stack(outputs, dim=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, tgt in tqdm(test_loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        preds = predict(model, src)\n",
        "\n",
        "        # Convert to numpy arrays for processing\n",
        "        src_np = src.cpu().numpy()\n",
        "        preds_np = preds.cpu().numpy()\n",
        "        tgt_np = tgt.cpu().numpy()\n",
        "\n",
        "        for i in range(len(src_np)):\n",
        "            # Get source, prediction and target sequences\n",
        "            s = src_np[i]\n",
        "            p = preds_np[i]\n",
        "            t = tgt_np[i]\n",
        "\n",
        "            # Store original sequences\n",
        "            all_src.append(s)\n",
        "            all_preds.append(p)\n",
        "            all_tgts.append(t)\n",
        "\n",
        "            # remove padding and everything after EOS\n",
        "            p_processed = []\n",
        "            for token in p:\n",
        "                if token == target_vocab['<eos>']:\n",
        "                    break\n",
        "                if token not in [target_vocab['<pad>'], target_vocab['<sos>']]:\n",
        "                    p_processed.append(token)\n",
        "\n",
        "            # remove padding and everything after EOS\n",
        "            t_processed = []\n",
        "            for token in t:\n",
        "                if token == target_vocab['<eos>']:\n",
        "                    break\n",
        "                if token not in [target_vocab['<pad>'], target_vocab['<sos>']]:\n",
        "                    t_processed.append(token)\n",
        "\n",
        "            # Compare the processed sequences\n",
        "            if p_processed == t_processed:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "accuracy = correct / total if total > 0 else 0\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Correct: {correct}, Total: {total}\")\n",
        "wandb.log({\"Test Accuracy\": accuracy})\n",
        "\n",
        "def log_sample_predictions_table_wandb(sources, preds, targets, idx2char_src, idx2char_tgt, num_samples=10):\n",
        "    table = wandb.Table(columns=[\"Source\", \"Prediction\", \"Reference\", \"status\"])\n",
        "\n",
        "    # Pick random indices without replacement\n",
        "    sample_indices = random.sample(range(len(sources)), min(num_samples, len(sources)))\n",
        "\n",
        "    for i in sample_indices:\n",
        "        src_word = ''.join([idx2char_src[idx] for idx in sources[i] if idx not in [source_vocab['<pad>'], source_vocab['<sos>'], source_vocab['<eos>']]])\n",
        "        pred_word = ''.join([idx2char_tgt[idx] for idx in preds[i] if idx not in [target_vocab['<pad>'], target_vocab['<sos>'], target_vocab['<eos>']]])\n",
        "        ref_word = ''.join([idx2char_tgt[idx] for idx in targets[i] if idx not in [target_vocab['<pad>'], target_vocab['<sos>'], target_vocab['<eos>']]])\n",
        "        # Determine correctness\n",
        "        is_correct = (pred_word == ref_word)\n",
        "        status = \"ðŸŸ© **Correct**\" if is_correct else \"ðŸŸ¥ **Incorrect**\"\n",
        "\n",
        "        table.add_data(src_word, pred_word, ref_word, status)\n",
        "\n",
        "    wandb.log({\"Test Sample Predictions (Color-Coded)\": table})\n",
        "\n",
        "log_sample_predictions_table_wandb(all_src, all_preds, all_tgts, idx2char_src, idx2char_tgt)\n",
        "\n",
        "output_dir = \"predictions_vanilla\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(output_dir, \"test_predictions.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for s, p, t in zip(all_src, all_preds, all_tgts):\n",
        "        src_word = ''.join([idx2char_src[idx] for idx in s if idx not in [source_vocab['<pad>'], source_vocab['<sos>'], source_vocab['<eos>']]])\n",
        "        pred_word = ''.join([idx2char_tgt[idx] for idx in p if idx not in [target_vocab['<pad>'], target_vocab['<sos>'], target_vocab['<eos>']]])\n",
        "        ref_word = ''.join([idx2char_tgt[idx] for idx in t if idx not in [target_vocab['<pad>'], target_vocab['<sos>'], target_vocab['<eos>']]])\n",
        "        f.write(f\"{src_word}\\t{pred_word}\\t{ref_word}\\n\")\n",
        "\n",
        "print(f\"Saved full predictions to: {output_dir}/test_predictions.txt\")\n",
        "wandb.save(os.path.join(output_dir, \"test_predictions.txt\"))\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "iRKRjEuYpjmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}