{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ia5xLHhoTWQKDQgDagMChE_Mlz4NpdbX",
      "authorship_tag": "ABX9TyPlcWMuQxIYxtSDC7IAGg90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sai-sakunthala/Assignment-3/blob/main/Assignment_3_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhSxPRJXneyD",
        "outputId": "605b7bb6-1f3a-4507-f4bf-ec7adeace183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install torch wandb pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import wandb\n",
        "import editdistance\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "LANG = 'te'\n",
        "data_path = f'/content/drive/MyDrive/dakshina_dataset_v1.0/{LANG}/lexicons/'\n",
        "\n",
        "def read_data(filepath, max_len=40):\n",
        "    pairs = []\n",
        "    with open(filepath, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            source, target = parts[0], parts[1]\n",
        "            if len(source) <= max_len and len(target) <= max_len:\n",
        "                pairs.append((source, target))\n",
        "    return pairs\n",
        "\n",
        "def make_vocab(sequences):\n",
        "    vocab = {'<pad>':0, '<sos>':1, '<eos>':2}\n",
        "    idx = 3\n",
        "    for seq in sequences:\n",
        "        for ch in seq:\n",
        "            if ch not in vocab:\n",
        "                vocab[ch] = idx\n",
        "                idx += 1\n",
        "    idx2char = {i:c for c,i in vocab.items()}\n",
        "    return vocab, idx2char\n",
        "\n",
        "def encode_word(word, vocab):\n",
        "    return [vocab['<sos>']] + [vocab[ch] for ch in word] + [vocab['<eos>']]\n",
        "\n",
        "def pad_seq(seq, max_len, pad_idx=0):\n",
        "    return seq + [pad_idx] * (max_len - len(seq))\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, pairs, source_vocab, target_vocab):\n",
        "        self.source_pad = source_vocab['<pad>']\n",
        "        self.target_pad = target_vocab['<pad>']\n",
        "        self.data = []\n",
        "        for source, target in pairs:\n",
        "            source_t = encode_word(source, source_vocab)\n",
        "            target_t = encode_word(target, target_vocab)\n",
        "            self.data.append((source_t, target_t))\n",
        "        self.source_max = max(len(x[0]) for x in self.data)\n",
        "        self.target_max = max(len(x[1]) for x in self.data)\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source, target = self.data[idx]\n",
        "        source = pad_seq(source, self.source_max, self.source_pad)\n",
        "        target = pad_seq(target, self.target_max, self.target_pad)\n",
        "        return torch.tensor(source), torch.tensor(target)\n",
        "\n",
        "class translit_Encoder(nn.Module):\n",
        "    def __init__(self, input_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell='lstm'):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dimensions, emb_dimensions)\n",
        "        rnn_cls = {'rnn': nn.RNN, 'gru': nn.GRU, 'lstm': nn.LSTM}[cell.lower()]\n",
        "        self.rnn = rnn_cls(emb_dimensions, hid_dimensions, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        self.cell = cell.lower()\n",
        "\n",
        "    def forward(self, source):\n",
        "        embedded = self.embedding(source)\n",
        "        if self.cell == 'lstm':\n",
        "            outputs, (hidden, cell) = self.rnn(embedded)\n",
        "            return hidden, cell\n",
        "        else:\n",
        "            outputs, hidden = self.rnn(embedded)\n",
        "            return hidden, None\n",
        "\n",
        "class translit_Decoder(nn.Module):\n",
        "    def __init__(self, output_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell='lstm'):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dimensions, emb_dimensions)\n",
        "        rnn_cls = {'rnn': nn.RNN, 'gru': nn.GRU, 'lstm': nn.LSTM}[cell.lower()]\n",
        "        self.rnn = rnn_cls(emb_dimensions, hid_dimensions, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hid_dimensions, output_dimensions)\n",
        "        self.cell = cell.lower()\n",
        "\n",
        "    def forward(self, input, hidden, cell=None):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.embedding(input)\n",
        "        if self.cell == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        else:\n",
        "            output, hidden = self.rnn(embedded, hidden)\n",
        "            cell = None\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class translit_Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.size(0)\n",
        "        target_len = target.size(1)\n",
        "        output_dimensions = self.decoder.fc_out.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, target_len, output_dimensions).to(self.device)\n",
        "        hidden, cell = self.encoder(source)\n",
        "        input = target[:, 0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = target[:, t] if teacher_force else top1\n",
        "        return outputs\n",
        "\n",
        "def strip_after_eos(seq, eos_idx):\n",
        "    \"\"\"Strip sequence after EOS token, handling both lists and numpy arrays\"\"\"\n",
        "    if isinstance(seq, np.ndarray):\n",
        "        eos_positions = np.where(seq == eos_idx)[0]\n",
        "        if len(eos_positions) > 0:\n",
        "            return seq[:eos_positions[0] + 1]\n",
        "        return seq\n",
        "    else:  # handle lists\n",
        "        if eos_idx in seq:\n",
        "            return seq[:seq.index(eos_idx) + 1]\n",
        "        return seq\n",
        "\n",
        "def calculate_word_accuracy(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        if eos_idx is not None:\n",
        "            pred = strip_after_eos(pred, eos_idx)\n",
        "            target = strip_after_eos(target, eos_idx)\n",
        "        pred = [p for p in pred if p != pad_idx]\n",
        "        target = [t for t in target if t != pad_idx]\n",
        "        if pred == target:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    return correct / total if total > 0 else 0\n",
        "\n",
        "def calculate_accuracy(preds, targets, pad_idx=0):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for p, t in zip(preds, targets):\n",
        "        for pi, ti in zip(p, t):\n",
        "            if ti == pad_idx:\n",
        "                continue\n",
        "            if pi == ti:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return correct / total if total > 0 else 0\n",
        "\n",
        "def calculate_cer(preds, targets, pad_idx=0):\n",
        "    cer, total = 0, 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        # Remove PAD tokens for evaluation.\n",
        "        pred = [p for p in pred if p != pad_idx]\n",
        "        target = [t for t in target if t != pad_idx]\n",
        "        cer += editdistance.eval(pred, target)\n",
        "        total += len(target)\n",
        "    return cer / total if total > 0 else 0"
      ],
      "metadata": {
        "id": "Sqry1mukh_mh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a34d25-4814-48fb-e682-5f7af351df6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "0EvudYf1GXOg",
        "outputId": "bf022004-37f3-4372-d2d9-22ede5b5c153"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msai-sakunthala\u001b[0m (\u001b[33msai-sakunthala-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##multi reference"
      ],
      "metadata": {
        "id": "eL6_up8aew3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this helper first to load all references\n",
        "from collections import defaultdict\n",
        "import unicodedata\n",
        "\n",
        "run = wandb.init(project=\"dakshina-seq2seq_2\", entity=\"sai-sakunthala-indian-institute-of-technology-madras\", name=\"evaluate_test_multi\")\n",
        "artifact = run.use_artifact('best_model:v6', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "# Read data and create vocabularies\n",
        "test_pairs = read_data(data_path + f\"{LANG}.translit.sampled.test.tsv\", max_len=30)\n",
        "train_pairs = read_data(data_path + f\"{LANG}.translit.sampled.train.tsv\", max_len=30)\n",
        "source_vocab, idx2char_src = make_vocab([x[0] for x in train_pairs])\n",
        "target_vocab, idx2char_tgt = make_vocab([x[1] for x in train_pairs])\n",
        "\n",
        "# Initialize model\n",
        "encoder = translit_Encoder(len(source_vocab), 256, 256*2, 2, 0.3, 'lstm').to(device)\n",
        "decoder = translit_Decoder(len(target_vocab), 256, 256*2, 2, 0.3, 'lstm').to(device)\n",
        "model = translit_Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "# Load model weights\n",
        "state_dict = torch.load(f\"{artifact_dir}/best_model.pt\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Create test dataset and loader\n",
        "test_translit = TransliterationDataset(test_pairs, source_vocab, target_vocab)\n",
        "test_loader = DataLoader(test_translit, batch_size=64, shuffle=False, drop_last=True)\n",
        "\n",
        "all_src, all_preds, all_tgts = [], [], []\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "def predict(model, src, max_len=30):\n",
        "    \"\"\"Greedy decoding implementation\"\"\"\n",
        "    encoder_hidden, encoder_cell = model.encoder(src)\n",
        "\n",
        "    # First input is SOS token\n",
        "    input = torch.tensor([target_vocab['<sos>']] * src.size(0)).to(device)\n",
        "    outputs = []\n",
        "\n",
        "    for t in range(max_len):\n",
        "        output, encoder_hidden, encoder_cell = model.decoder(input, encoder_hidden, encoder_cell)\n",
        "        input = output.argmax(1)  # Greedy decoding\n",
        "        outputs.append(input)\n",
        "\n",
        "        # Stop if all sequences predicted EOS\n",
        "        if (input == target_vocab['<eos>']).all():\n",
        "            break\n",
        "\n",
        "    return torch.stack(outputs, dim=1)\n",
        "\n",
        "def normalize(text):\n",
        "    return unicodedata.normalize('NFC', text)\n",
        "\n",
        "def read_test_refs(filepath, max_len=30):\n",
        "    ref_dict = defaultdict(list)\n",
        "    with open(filepath, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            # Accept lines with 2 or 3 parts (ignore the third if present)\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            src, tgt = parts[0], parts[1]\n",
        "            if len(src) <= max_len and len(tgt) <= max_len:\n",
        "                ref_dict[normalize(src)].append(normalize(tgt))\n",
        "    return ref_dict\n",
        "\n",
        "# Use the reference loader to get valid outputs\n",
        "ref_dict = read_test_refs(data_path + f\"{LANG}.translit.sampled.test.tsv\", max_len=30)\n",
        "\n",
        "# Create a function to convert int sequences to string\n",
        "\n",
        "def tokens_to_string(token_seq, idx2char, vocab):\n",
        "    tokens = []\n",
        "    for tok in token_seq:\n",
        "        if tok in [vocab['<pad>'], vocab['<sos>'], vocab['<eos>']]:\n",
        "            continue\n",
        "        tokens.append(idx2char[tok])\n",
        "    return ''.join(tokens)\n",
        "\n",
        "# Replace the accuracy calculation loop\n",
        "correct = 0\n",
        "all_src, all_preds, all_refs = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, tgt in tqdm(test_loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        preds = predict(model, src)\n",
        "\n",
        "        src_np = src.cpu().numpy()\n",
        "        preds_np = preds.cpu().numpy()\n",
        "\n",
        "        for i in range(len(src_np)):\n",
        "            src_word = normalize(tokens_to_string(src_np[i], idx2char_src, source_vocab))\n",
        "            pred_word = normalize(tokens_to_string(preds_np[i], idx2char_tgt, target_vocab))\n",
        "            valid_refs = ref_dict.get(src_word, [])\n",
        "\n",
        "            all_src.append(src_np[i])\n",
        "            all_preds.append(preds_np[i])\n",
        "            all_refs.append(valid_refs)\n",
        "            if pred_word in valid_refs:\n",
        "                correct += 1\n",
        "\n",
        "accuracy = correct / len(all_src)\n",
        "print(f\"Test Accuracy (Multi-Ref): {accuracy:.4f}\")\n",
        "wandb.log({\"Test Accuracy (Multi-Ref)\": accuracy})\n",
        "\n",
        "# Update table logging to handle multiple references\n",
        "\n",
        "def log_sample_predictions_table_wandb(sources, preds, targets, idx2char_src, idx2char_tgt, num_samples=10):\n",
        "    table = wandb.Table(columns=[\"Source\", \"Prediction\", \"Valid Reference(s)\", \"Correct?\"])\n",
        "    sample_indices = random.sample(range(len(sources)), min(num_samples, len(sources)))\n",
        "\n",
        "    for i in sample_indices:\n",
        "        src_word = normalize(tokens_to_string(sources[i], idx2char_src, source_vocab))\n",
        "        pred_word = normalize(tokens_to_string(preds[i], idx2char_tgt, target_vocab))\n",
        "        valid_refs = ref_dict.get(src_word, [])\n",
        "        ref_display = ', '.join(valid_refs)\n",
        "\n",
        "        # Determine correctness\n",
        "        is_correct = pred_word in valid_refs\n",
        "        status = \"ğŸŸ© **Correct**\" if is_correct else \"ğŸŸ¥ **Incorrect**\"\n",
        "\n",
        "        table.add_data(src_word, pred_word, ref_display, status)\n",
        "\n",
        "    wandb.log({\"Test Sample Predictions (Color-Coded)\": table})\n",
        "\n",
        "log_sample_predictions_table_wandb(all_src, all_preds, ref_dict, idx2char_src, idx2char_tgt)\n",
        "\n",
        "output_dir = \"predictions_vanilla\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save updated predictions\n",
        "with open(os.path.join(output_dir, \"test_predictions.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for s, p in zip(all_src, all_preds):\n",
        "        src_word = normalize(tokens_to_string(s, idx2char_src, source_vocab))\n",
        "        pred_word = normalize(tokens_to_string(p, idx2char_tgt, target_vocab))\n",
        "        refs = ', '.join(ref_dict.get(src_word, []))\n",
        "        f.write(f\"{src_word}\\t{pred_word}\\t{refs}\\n\")\n",
        "wandb.save(os.path.join(output_dir, \"test_predictions.txt\"))\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "maJ4cgDzV3aL",
        "outputId": "133b9cb8-1840-4d99-8107-3e1654d88ce1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250519_124923-mab8tuxo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/mab8tuxo' target=\"_blank\">evaluate_test_multi</a></strong> to <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/mab8tuxo' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/mab8tuxo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [00:45<00:00,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (Multi-Ref): 0.7049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy (Multi-Ref)</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy (Multi-Ref)</td><td>0.70488</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">evaluate_test_multi</strong> at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/mab8tuxo' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/mab8tuxo</a><br> View project at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250519_124923-mab8tuxo/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "Nt_6ztjohpqd",
        "outputId": "663b69a0-dff1-4ef2-d0ab-4ce92f543622"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy (Multi-Ref)</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy (Multi-Ref)</td><td>0.70488</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">evaluate_test_multi</strong> at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/nbxsil8l' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/nbxsil8l</a><br> View project at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250519_122957-nbxsil8l/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## no multi reference"
      ],
      "metadata": {
        "id": "JmvZSgm7e1Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(project=\"dakshina-seq2seq_2\", entity=\"sai-sakunthala-indian-institute-of-technology-madras\", name=\"evaluate_test\")\n",
        "artifact = run.use_artifact('best_model:v6', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "# Read data and create vocabularies\n",
        "test_pairs = read_data(data_path + f\"{LANG}.translit.sampled.test.tsv\", max_len=30)\n",
        "train_pairs = read_data(data_path + f\"{LANG}.translit.sampled.train.tsv\", max_len=30)\n",
        "source_vocab, idx2char_src = make_vocab([x[0] for x in train_pairs])\n",
        "target_vocab, idx2char_tgt = make_vocab([x[1] for x in train_pairs])\n",
        "\n",
        "# Initialize model\n",
        "encoder = translit_Encoder(len(source_vocab), 256, 256*2, 2, 0.3, 'lstm').to(device)\n",
        "decoder = translit_Decoder(len(target_vocab), 256, 256*2, 2, 0.3, 'lstm').to(device)\n",
        "model = translit_Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "# Load model weights\n",
        "state_dict = torch.load(f\"{artifact_dir}/best_model.pt\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Create test dataset and loader\n",
        "test_translit = TransliterationDataset(test_pairs, source_vocab, target_vocab)\n",
        "test_loader = DataLoader(test_translit, batch_size=64, shuffle=False, drop_last=True)\n",
        "\n",
        "all_src, all_preds, all_tgts = [], [], []\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "def predict(model, src, max_len=30):\n",
        "    \"\"\"Greedy decoding implementation\"\"\"\n",
        "    encoder_hidden, encoder_cell = model.encoder(src)\n",
        "\n",
        "    # First input is SOS token\n",
        "    input = torch.tensor([target_vocab['<sos>']] * src.size(0)).to(device)\n",
        "    outputs = []\n",
        "\n",
        "    for t in range(max_len):\n",
        "        output, encoder_hidden, encoder_cell = model.decoder(input, encoder_hidden, encoder_cell)\n",
        "        input = output.argmax(1)  # Greedy decoding\n",
        "        outputs.append(input)\n",
        "\n",
        "        # Stop if all sequences predicted EOS\n",
        "        if (input == target_vocab['<eos>']).all():\n",
        "            break\n",
        "\n",
        "    return torch.stack(outputs, dim=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, tgt in tqdm(test_loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        preds = predict(model, src)\n",
        "\n",
        "        # Convert to numpy arrays for processing\n",
        "        src_np = src.cpu().numpy()\n",
        "        preds_np = preds.cpu().numpy()\n",
        "        tgt_np = tgt.cpu().numpy()\n",
        "\n",
        "        for i in range(len(src_np)):\n",
        "            # Get source, prediction and target sequences\n",
        "            s = src_np[i]\n",
        "            p = preds_np[i]\n",
        "            t = tgt_np[i]\n",
        "\n",
        "            # Store original sequences\n",
        "            all_src.append(s)\n",
        "            all_preds.append(p)\n",
        "            all_tgts.append(t)\n",
        "\n",
        "            # Process prediction: remove padding and everything after EOS\n",
        "            p_processed = []\n",
        "            for token in p:\n",
        "                if token == target_vocab['<eos>']:\n",
        "                    break\n",
        "                if token not in [target_vocab['<pad>'], target_vocab['<sos>']]:\n",
        "                    p_processed.append(token)\n",
        "\n",
        "            # Process target: remove padding and everything after EOS\n",
        "            t_processed = []\n",
        "            for token in t:\n",
        "                if token == target_vocab['<eos>']:\n",
        "                    break\n",
        "                if token not in [target_vocab['<pad>'], target_vocab['<sos>']]:\n",
        "                    t_processed.append(token)\n",
        "\n",
        "            # Compare the processed sequences\n",
        "            if p_processed == t_processed:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "accuracy = correct / total if total > 0 else 0\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Correct: {correct}, Total: {total}\")\n",
        "wandb.log({\"Test Accuracy\": accuracy})\n",
        "\n",
        "def log_sample_predictions_table_wandb(sources, preds, targets, idx2char_src, idx2char_tgt, num_samples=10):\n",
        "    table = wandb.Table(columns=[\"Source\", \"Prediction\", \"Reference\"])\n",
        "\n",
        "    # Pick random indices without replacement\n",
        "    sample_indices = random.sample(range(len(sources)), min(num_samples, len(sources)))\n",
        "\n",
        "    for i in sample_indices:\n",
        "        src_word = ''.join([idx2char_src[idx] for idx in sources[i] if idx not in [source_vocab['<pad>'], source_vocab['<sos>'], source_vocab['<eos>']]])\n",
        "        pred_word = ''.join([idx2char_tgt[idx] for idx in preds[i] if idx not in [target_vocab['<pad>'], target_vocab['<sos>'], target_vocab['<eos>']]])\n",
        "        ref_word = ''.join([idx2char_tgt[idx] for idx in targets[i] if idx not in [target_vocab['<pad>'], target_vocab['<sos>'], target_vocab['<eos>']]])\n",
        "        table.add_data(src_word, pred_word, ref_word)\n",
        "\n",
        "    wandb.log({\"Test Sample Predictions Table\": table})\n",
        "\n",
        "log_sample_predictions_table_wandb(all_src, all_preds, all_tgts, idx2char_src, idx2char_tgt)\n",
        "\n",
        "output_dir = \"predictions_vanilla\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(output_dir, \"test_predictions.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for s, p, t in zip(all_src, all_preds, all_tgts):\n",
        "        src_word = ''.join([idx2char_src[idx] for idx in s if idx not in [source_vocab['<pad>'], source_vocab['<sos>'], source_vocab['<eos>']]])\n",
        "        pred_word = ''.join([idx2char_tgt[idx] for idx in p if idx not in [target_vocab['<pad>'], target_vocab['<sos>'], target_vocab['<eos>']]])\n",
        "        ref_word = ''.join([idx2char_tgt[idx] for idx in t if idx not in [target_vocab['<pad>'], target_vocab['<sos>'], target_vocab['<eos>']]])\n",
        "        f.write(f\"{src_word}\\t{pred_word}\\t{ref_word}\\n\")\n",
        "\n",
        "print(f\"Saved full predictions to: {output_dir}/test_predictions.txt\")\n",
        "wandb.save(os.path.join(output_dir, \"test_predictions.txt\"))\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "id": "iRKRjEuYpjmB",
        "outputId": "d230b974-7344-43e3-ab81-ae0a11f1373a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy (Multi-Ref)</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy (Multi-Ref)</td><td>0.70488</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">evaluate_test</strong> at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/0gidz4jd' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/0gidz4jd</a><br> View project at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250519_121613-0gidz4jd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250519_122109-b0elx7d4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/b0elx7d4' target=\"_blank\">evaluate_test</a></strong> to <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/b0elx7d4' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/b0elx7d4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [00:45<00:00,  1.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.3278\n",
            "Correct: 1867, Total: 5696\n",
            "Saved full predictions to: predictions_vanilla/test_predictions.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.32777</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">evaluate_test</strong> at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/b0elx7d4' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2/runs/b0elx7d4</a><br> View project at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq_2</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250519_122109-b0elx7d4/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}