{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jRBG1VKWj0Ud3f08_YheH-myhuwgVtUp",
      "authorship_tag": "ABX9TyPepvnkY83ovlobSk9Y+RZ6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hta9vgP-WmaL"
      },
      "outputs": [],
      "source": [
        "pip install torch wandb pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import wandb\n",
        "import editdistance\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "LANG = 'te'\n",
        "data_path = f'/content/drive/MyDrive/dakshina_dataset_v1.0/{LANG}/lexicons/'\n",
        "\n",
        "def read_data(filepath, max_len=40):\n",
        "    pairs = []\n",
        "    # Open the file with UTF-8 encoding to properly read Unicode characters\n",
        "    with open(filepath, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            # Remove leading/trailing whitespace and split by tab\n",
        "            parts = line.strip().split('\\t')\n",
        "            # Skip lines that don't contain both source and target text\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            devanagiri, latin = parts[0], parts[1]\n",
        "\n",
        "            # We are training a Latin → Devanagiri transliteration model,\n",
        "            # so set Latin as the source and Devanagiri as the target\n",
        "            source, target = latin, devanagiri\n",
        "\n",
        "            # Only keep pairs where both source and target are within the allowed max length\n",
        "            if len(source) <= max_len and len(target) <= max_len:\n",
        "                pairs.append((source, target))\n",
        "\n",
        "    # Return the list of filtered (source, target) pairs\n",
        "    return pairs\n",
        "\n",
        "def make_vocab(sequences):\n",
        "    # Initialize the vocabulary with special tokens\n",
        "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n",
        "    idx = 3  # Starting index for regular characters\n",
        "\n",
        "    # Loop through all sequences to build the vocabulary\n",
        "    for seq in sequences:\n",
        "        for ch in seq:\n",
        "            # Add each unique character to the vocabulary\n",
        "            if ch not in vocab:\n",
        "                vocab[ch] = idx\n",
        "                idx += 1\n",
        "\n",
        "    # Create reverse mapping from index to character\n",
        "    idx2char = {i: c for c, i in vocab.items()}\n",
        "\n",
        "    # Return both the character-to-index and index-to-character dictionaries\n",
        "    return vocab, idx2char\n",
        "\n",
        "def encode_word(word, vocab):\n",
        "    # Convert a word into a list of indices using the vocabulary\n",
        "    # Add <sos> token at the beginning and <eos> token at the end\n",
        "    return [vocab['<sos>']] + [vocab[ch] for ch in word] + [vocab['<eos>']]\n",
        "\n",
        "def pad_seq(seq, max_len, pad_idx=0):\n",
        "    # Pad the sequence with <pad> tokens (default index 0) to reach max_len\n",
        "    return seq + [pad_idx] * (max_len - len(seq))\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, pairs, source_vocab, target_vocab):\n",
        "        # Save padding indices for both source and target vocabularies\n",
        "        self.source_pad = source_vocab['<pad>']\n",
        "        self.target_pad = target_vocab['<pad>']\n",
        "        self.data = []\n",
        "\n",
        "        # Convert each (source, target) word pair into sequences of token indices\n",
        "        for source, target in pairs:\n",
        "            source_t = encode_word(source, source_vocab)\n",
        "            target_t = encode_word(target, target_vocab)\n",
        "            self.data.append((source_t, target_t))\n",
        "\n",
        "        # Determine the maximum lengths of source and target sequences\n",
        "        self.source_max = max(len(x[0]) for x in self.data)\n",
        "        self.target_max = max(len(x[1]) for x in self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return total number of samples in the dataset\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch a source-target pair and pad both to their respective max lengths\n",
        "        source, target = self.data[idx]\n",
        "        source = pad_seq(source, self.source_max, self.source_pad)\n",
        "        target = pad_seq(target, self.target_max, self.target_pad)\n",
        "        return torch.tensor(source), torch.tensor(target)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dimensions):\n",
        "        super().__init__()\n",
        "        # Linear layer to compute attention scores from hidden and encoder outputs\n",
        "        self.attn = nn.Linear(hid_dimensions * 2, hid_dimensions)\n",
        "\n",
        "        # Learnable vector used to reduce the attention scores to a scalar\n",
        "        self.v = nn.Parameter(torch.rand(hid_dimensions))\n",
        "\n",
        "        # Initialize vector weights uniformly\n",
        "        stdv = 1. / (hid_dimensions ** 0.5)\n",
        "        self.v.data.uniform_(-stdv, stdv)\n",
        "\n",
        "        self.hid_dimensions = hid_dimensions\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: decoder hidden state\n",
        "        # encoder_outputs: all encoder outputs for the input sequence\n",
        "\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        src_len = encoder_outputs.size(1)\n",
        "\n",
        "        # If hidden state has multiple layers, take the last one\n",
        "        if hidden.dim() == 3:\n",
        "            hidden = hidden[-1]\n",
        "        elif hidden.dim() != 2:\n",
        "            raise ValueError(f\"Expected hidden to be 2D or 3D, got shape {hidden.shape}\")\n",
        "\n",
        "        # Repeat hidden state to match the number of encoder outputs\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        # Concatenate hidden and encoder outputs, then pass through a non-linear layer\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "\n",
        "        # Compute raw attention scores using the learnable vector `v`\n",
        "        energy = energy @ self.v\n",
        "\n",
        "        # Normalize scores into a probability distribution (attention weights)\n",
        "        attn_weights = torch.softmax(energy, dim=1).unsqueeze(2)\n",
        "\n",
        "        # Compute weighted sum of encoder outputs (context vector)\n",
        "        context = torch.sum(attn_weights * encoder_outputs, dim=1)\n",
        "\n",
        "        # Return both the context vector and the attention weights\n",
        "        return context, attn_weights.squeeze(2)\n",
        "\n",
        "class translit_Decoder(nn.Module):\n",
        "    def __init__(self, output_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell='lstm'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer to convert token indices into dense vectors\n",
        "        self.embedding = nn.Embedding(output_dimensions, emb_dimensions)\n",
        "\n",
        "        # Attention module to focus on relevant parts of the encoder output\n",
        "        self.attention = Attention(hid_dimensions)\n",
        "\n",
        "        # Choose RNN type based on user-specified cell type\n",
        "        rnn_cls = {'rnn': nn.RNN, 'gru': nn.GRU, 'lstm': nn.LSTM}[cell.lower()]\n",
        "\n",
        "        # RNN layer to process embedded inputs and context\n",
        "        self.rnn = rnn_cls(\n",
        "            emb_dimensions, hid_dimensions, num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Final fully connected layer to map combined context + RNN output to vocabulary logits\n",
        "        self.fc_out = nn.Linear(hid_dimensions * 2, output_dimensions)\n",
        "\n",
        "        # Store the type of RNN cell\n",
        "        self.cell = cell.lower()\n",
        "\n",
        "        # Apply dropout to the embeddings\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        # Add time dimension to input (batch_size → batch_size x 1)\n",
        "        input = input.unsqueeze(1)\n",
        "\n",
        "        # Convert input token index to embedding and apply dropout\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        # Pass through the RNN (handle LSTM and others differently)\n",
        "        if self.cell == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        else:\n",
        "            output, hidden = self.rnn(embedded, hidden)\n",
        "            cell = None  # Non-LSTM cells don't return a separate cell state\n",
        "\n",
        "        # Use attention mechanism to compute context vector from encoder outputs\n",
        "        context, attn_weights = self.attention(hidden, encoder_outputs)\n",
        "\n",
        "        # Remove time dimension from RNN output\n",
        "        rnn_output = output.squeeze(1)\n",
        "\n",
        "        # Combine RNN output and context for final prediction\n",
        "        combined = torch.cat((rnn_output, context), dim=1)\n",
        "\n",
        "        # Compute the predicted output token scores\n",
        "        prediction = self.fc_out(combined)\n",
        "\n",
        "        # Return prediction, updated hidden/cell states, and attention weights\n",
        "        return prediction, hidden, cell, attn_weights\n",
        "\n",
        "\n",
        "class translit_Encoder(nn.Module):\n",
        "    def __init__(self, input_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell='lstm'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer to convert input indices into dense vectors\n",
        "        self.embedding = nn.Embedding(input_dimensions, emb_dimensions)\n",
        "\n",
        "        # Choose RNN type based on cell argument\n",
        "        rnn_cls = {'rnn': nn.RNN, 'gru': nn.GRU, 'lstm': nn.LSTM}[cell.lower()]\n",
        "\n",
        "        # RNN layer to process the embedded input sequence\n",
        "        self.rnn = rnn_cls(\n",
        "            emb_dimensions, hid_dimensions, num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Store attention module and cell type\n",
        "        self.attention = Attention(hid_dimensions)\n",
        "        self.cell = cell.lower()\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, source):\n",
        "        # Convert input token indices into embeddings and apply dropout\n",
        "        embedded = self.dropout(self.embedding(source))\n",
        "\n",
        "        # Pass embedded input through RNN\n",
        "        if self.cell == 'lstm':\n",
        "            outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        else:\n",
        "            outputs, hidden = self.rnn(embedded)\n",
        "            cell = None\n",
        "\n",
        "        # Compute context using attention (optional, can be ignored in basic encoder usage)\n",
        "        context = self.attention(hidden, outputs)\n",
        "\n",
        "        # Return the full sequence of encoder outputs, last hidden state, and cell state (if any)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class translit_Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder        # Encoder processes the input sequence\n",
        "        self.decoder = decoder        # Decoder generates the output sequence\n",
        "        self.device = device          # Device on which computation is performed (CPU/GPU)\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.size(0)\n",
        "        target_len = target.size(1)\n",
        "        output_dimensions = self.decoder.fc_out.out_features\n",
        "\n",
        "        # Initialize tensor to store decoder predictions for each time step\n",
        "        outputs = torch.zeros(batch_size, target_len, output_dimensions).to(self.device)\n",
        "\n",
        "        # Initialize tensor to keep track of attention weights over time\n",
        "        attn_weights_all = torch.zeros(batch_size, target_len, source.size(1)).to(self.device)\n",
        "\n",
        "        # Run the encoder on the source sequence to get hidden states\n",
        "        encoder_outputs, hidden, cell = self.encoder(source)\n",
        "\n",
        "        # Set initial decoder input to the <sos> token\n",
        "        input = target[:, 0]\n",
        "\n",
        "        # Loop over each time step in the target sequence\n",
        "        for t in range(1, target_len):\n",
        "            # Get decoder output and updated hidden states\n",
        "            output, hidden, cell, attn_weights = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "\n",
        "            # Store the current output prediction\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            # Save attention weights for this time step\n",
        "            attn_weights_all[:, t] = attn_weights\n",
        "\n",
        "            # Decide whether to use ground truth or model prediction for next input\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = target[:, t] if teacher_force else top1\n",
        "\n",
        "        # Return the full sequence of predictions and attention weights\n",
        "        return outputs, attn_weights_all\n",
        "\n",
        "\n",
        "def strip_after_eos(seq, eos_idx):\n",
        "    # Convert tensor to list if needed\n",
        "    if isinstance(seq, torch.Tensor):\n",
        "        seq = seq.cpu().numpy().tolist()\n",
        "    # Trim the sequence at the first <eos> token\n",
        "    if eos_idx in seq:\n",
        "        return seq[:seq.index(eos_idx)]\n",
        "    return seq\n",
        "\n",
        "def calculate_word_accuracy(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    correct = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        # Remove padding and stop at <eos> for fair comparison\n",
        "        pred = strip_after_eos(pred, eos_idx) if eos_idx else pred\n",
        "        target = strip_after_eos(target, eos_idx) if eos_idx else target\n",
        "        pred = [p for p in pred if p != pad_idx]\n",
        "        target = [t for t in target if t != pad_idx]\n",
        "        # Count if full predicted word matches target\n",
        "        correct += int(pred == target)\n",
        "    return correct / max(len(preds), 1)\n",
        "\n",
        "\n",
        "def calculate_cer(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    cer = 0\n",
        "    total = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        # Clean sequences by removing padding and trimming after <eos>\n",
        "        pred = strip_after_eos(pred, eos_idx) if eos_idx else pred\n",
        "        target = strip_after_eos(target, eos_idx) if eos_idx else target\n",
        "        pred = [p for p in pred if p != pad_idx]\n",
        "        target = [t for t in target if t != pad_idx]\n",
        "        # Accumulate edit distance and total characters\n",
        "        cer += editdistance.eval(pred, target)\n",
        "        total += max(len(target), 1)\n",
        "    return cer / total if total > 0 else float('inf')\n",
        "\n",
        "\n",
        "def calculate_accuracy(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        # Convert tensors to lists if necessary\n",
        "        if isinstance(pred, torch.Tensor):\n",
        "            pred = pred.cpu().tolist()\n",
        "        if isinstance(target, torch.Tensor):\n",
        "            target = target.cpu().tolist()\n",
        "        # Strip <eos> tokens if specified\n",
        "        if eos_idx is not None:\n",
        "            pred = strip_after_eos(pred, eos_idx)\n",
        "            target = strip_after_eos(target, eos_idx)\n",
        "        # Compare tokens one by one, ignoring padding\n",
        "        for p_token, t_token in zip(pred, target):\n",
        "            if t_token == pad_idx:\n",
        "                continue\n",
        "            if p_token == t_token:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return correct / total if total > 0 else 0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqP_qshcWqfD",
        "outputId": "17b68f21-b0f0-4779-e157-ddf85d2446b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dash\n",
        "!pip install plotly"
      ],
      "metadata": {
        "id": "hJooYdyPw6Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "3g9F1-ANrbbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "run = wandb.init(project=\"attention-viz-2\", entity=\"sai-sakunthala-indian-institute-of-technology-madras\", name=\"evaluate_test\")\n",
        "artifact = run.use_artifact('sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq-3/best_model:v48', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "run.finish()\n",
        "\n",
        "# Read data and create vocabularies\n",
        "test_pairs = read_data(data_path + f\"{LANG}.translit.sampled.test.tsv\", max_len=30)\n",
        "train_pairs = read_data(data_path + f\"{LANG}.translit.sampled.train.tsv\", max_len=30)\n",
        "source_vocab, idx2char_src = make_vocab([x[0] for x in train_pairs])\n",
        "target_vocab, idx2char_tgt = make_vocab([x[1] for x in train_pairs])\n",
        "\n",
        "# Model parameters (must match training)\n",
        "input_dimensions = len(source_vocab)\n",
        "output_dimensions = len(target_vocab)\n",
        "emb_dimensions = 256\n",
        "hid_dimensions = 256 * 2\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "cell = 'lstm'\n",
        "batch_size = 64\n",
        "max_len = 30\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize model\n",
        "encoder = translit_Encoder(input_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell).to(device)\n",
        "decoder = translit_Decoder(output_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell).to(device)\n",
        "model = translit_Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "# Load model weights\n",
        "state_dict = torch.load(f\"{artifact_dir}/best_model.pt\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Create test dataset and loader\n",
        "test_translit = TransliterationDataset(test_pairs, source_vocab, target_vocab)\n",
        "test_loader = DataLoader(test_translit, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "all_src, all_preds, all_tgts, all_attn_weights = [], [], [], []\n",
        "correct = 0\n",
        "total = 0\n",
        "selected_examples = []\n",
        "\n",
        "def predict(model, src, max_len=30):\n",
        "\n",
        "    encoder_outputs, encoder_hidden, encoder_cell = model.encoder(src)\n",
        "    input = torch.tensor([target_vocab['<sos>']] * src.size(0)).to(device)\n",
        "    outputs = []\n",
        "    attn_weights_list = []\n",
        "\n",
        "    for t in range(max_len):\n",
        "        output, encoder_hidden, encoder_cell, attn_weights = model.decoder(input, encoder_hidden, encoder_cell, encoder_outputs)\n",
        "        input = output.argmax(1)\n",
        "        outputs.append(input)\n",
        "        attn_weights_list.append(attn_weights)\n",
        "\n",
        "        if (input == target_vocab.get('<eos>', -1)).all():\n",
        "            break\n",
        "\n",
        "    outputs = torch.stack(outputs, dim=1)  # (batch_size, max_len)\n",
        "    attn_weights_all = torch.stack(attn_weights_list, dim=1)  # (batch_size, max_len, src_len)\n",
        "    return outputs, attn_weights_all\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, tgt in tqdm(test_loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        preds, attn_weights_batch = predict(model, src)\n",
        "\n",
        "        # Convert to numpy arrays for processing\n",
        "        src_np = src.cpu().numpy()\n",
        "        preds_np = preds.cpu().numpy()\n",
        "        tgt_np = tgt.cpu().numpy()\n",
        "        attn_weights_np = attn_weights_batch.cpu().numpy()\n",
        "\n",
        "        for i in range(len(src_np)):\n",
        "            # Get source, prediction, target, and attention weights\n",
        "            s = src_np[i]\n",
        "            p = preds_np[i]\n",
        "            t = tgt_np[i]\n",
        "            attn = attn_weights_np[i]\n",
        "\n",
        "            # Store sequences and attention weights for all examples\n",
        "            all_src.append(s)\n",
        "            all_preds.append(p)\n",
        "            all_tgts.append(t)\n",
        "            all_attn_weights.append(attn)\n",
        "\n",
        "            # Collect up to 12 examples for heatmaps\n",
        "            if len(selected_examples) < 12:\n",
        "                selected_examples.append((s, p, t, attn))\n",
        "\n",
        "            # Process prediction: remove padding and everything after EOS\n",
        "            p_processed = []\n",
        "            for token in p:\n",
        "                if token == target_vocab.get('<eos>', -1):\n",
        "                    break\n",
        "                if token not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1)]:\n",
        "                    p_processed.append(token)\n",
        "\n",
        "            # Process target: remove padding and everything after EOS\n",
        "            t_processed = []\n",
        "            for token in t:\n",
        "                if token == target_vocab.get('<eos>', -1):\n",
        "                    break\n",
        "                if token not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1)]:\n",
        "                    t_processed.append(token)\n",
        "\n",
        "            # Compare the processed sequences\n",
        "            if p_processed == t_processed:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct / total if total > 0 else 0\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Correct: {correct}, Total: {total}\")"
      ],
      "metadata": {
        "id": "LJ4TX8mGsMzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store correctly predicted samples\n",
        "correctly_predicted_samples = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, tgt in tqdm(test_loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        preds, attn_weights_batch = predict(model, src)\n",
        "\n",
        "        # Convert to numpy arrays for processing\n",
        "        src_np = src.cpu().numpy()\n",
        "        preds_np = preds.cpu().numpy()\n",
        "        tgt_np = tgt.cpu().numpy()\n",
        "        attn_weights_np = attn_weights_batch.cpu().numpy()\n",
        "\n",
        "        for i in range(len(src_np)):\n",
        "            # Get source, prediction, target, and attention weights\n",
        "            s = src_np[i]\n",
        "            p = preds_np[i]\n",
        "            t = tgt_np[i]\n",
        "            attn = attn_weights_np[i]\n",
        "\n",
        "            # Process prediction: remove padding and everything after EOS\n",
        "            p_processed = []\n",
        "            for token in p:\n",
        "                if token == target_vocab.get('<eos>', -1):\n",
        "                    break\n",
        "                if token not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1)]:\n",
        "                    p_processed.append(token)\n",
        "\n",
        "            # Process target: remove padding and everything after EOS\n",
        "            t_processed = []\n",
        "            for token in t:\n",
        "                if token == target_vocab.get('<eos>', -1):\n",
        "                    break\n",
        "                if token not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1)]:\n",
        "                    t_processed.append(token)\n",
        "\n",
        "            # Compare the processed sequences\n",
        "            if p_processed == t_processed:\n",
        "                correctly_predicted_samples.append((s, p, t, attn))  # Store the correct prediction\n",
        "\n",
        "                # Stop if we have collected 4 correct predictions\n",
        "                if len(correctly_predicted_samples) >= 10:\n",
        "                    break\n",
        "\n",
        "        # Break the outer loop if we have enough correct predictions\n",
        "        if len(correctly_predicted_samples) >= 10:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utbtvT3MLru2",
        "outputId": "725cb326-dfaf-4fff-e611-85bbb9e22824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/89 [00:03<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualization_data = []\n",
        "selected_examples = correctly_predicted_samples\n",
        "for i in range(len(selected_examples)):\n",
        "    s, p, t, attn = selected_examples[i]\n",
        "\n",
        "    # Convert indices to characters\n",
        "    src_word = ''.join([idx2char_src.get(idx, '?') for idx in s if idx not in [source_vocab.get('<pad>', -1), source_vocab.get('<sos>', -1), source_vocab.get('<eos>', -1)]])\n",
        "    pred_word = ''.join([idx2char_tgt.get(idx, '?') for idx in p if idx not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1), target_vocab.get('<eos>', -1)]])\n",
        "    ref_word = ''.join([idx2char_tgt.get(idx, '?') for idx in t if idx not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1), target_vocab.get('<eos>', -1)]])\n",
        "\n",
        "    # Find the source characters with maximum attention for each target character\n",
        "    max_attention_chars = []\n",
        "    for attn_weights in attn:\n",
        "        max_index = attn_weights.argmax()  # Get the index of the max attention weight\n",
        "        max_attention_char = idx2char_src.get(max_index, '?')  # Get the corresponding source character\n",
        "        max_attention_chars.append(max_attention_char)\n",
        "    # Store the visualization data\n",
        "    visualization_data.append({\n",
        "        \"src_word\": src_word,\n",
        "        \"pred_word\": pred_word,\n",
        "        \"ref_word\": ref_word,\n",
        "        \"max_attention_chars\": max_attention_chars\n",
        "    })\n",
        "\n",
        "# Example of how to print or log the visualization data\n",
        "for data in visualization_data:\n",
        "    print(f\"Source: {data['src_word']}, Predicted: {data['pred_word']}, Reference: {data['ref_word']}, Max Attention: {''.join(data['max_attention_chars'])}\")"
      ],
      "metadata": {
        "id": "MqWch0QoJ2Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import json\n",
        "import base64\n",
        "import os\n",
        "\n",
        "# Initialize W&B\n",
        "run = wandb.init(project=\"attention-viz-2\", entity=\"sai-sakunthala-indian-institute-of-technology-madras\", name=\"interactive_image_visualization\")\n",
        "\n",
        "visualization_data = [\n",
        "    {\"src_word\":\"amkamlo\",\"pred_word\":\"అంకంలో\",\"ref_word\":\"అంకంలో\",\"max_attention_chars\":\"&lt;sos&gt;amitthhh\"},\n",
        "    {\"src_word\":\"ankamlo\",\"pred_word\":\"అంకంలో\",\"ref_word\":\"అంకాలో\",\"max_attention_chars\":\"&lt;sos&gt;amitthhhh\"},\n",
        "    {\"src_word\":\"ankamloo\",\"pred_word\":\"అంకంలో\",\"ref_word\":\"అంకంలో\",\"max_attention_chars\":\"&lt;sos&gt;amitnbbb\"},\n",
        "    {\"src_word\":\"amkitamai\",\"pred_word\":\"అంకితమై\",\"ref_word\":\"అంకితమై\",\"max_attention_chars\":\"&lt;sos&gt;amkinbccc\"},\n",
        "    {\"src_word\":\"ankitamai\",\"pred_word\":\"అంకితమై\",\"ref_word\":\"అంకితమై\",\"max_attention_chars\":\"&lt;sos&gt;aakknbccc\"},\n",
        "    {\"src_word\":\"ankela\",\"pred_word\":\"అంకెల\",\"ref_word\":\"అంకెల\",\"max_attention_chars\":\"&lt;sos&gt;aakittt\"},\n",
        "    {\"src_word\":\"ankelanu\",\"pred_word\":\"అంకెలను\",\"ref_word\":\"అంకెలను\",\"max_attention_chars\":\"&lt;sos&gt;aakithbb\"},\n",
        "    {\"src_word\":\"angeekarinchaka\",\"pred_word\":\"అంగీకరించక\",\"ref_word\":\"అంగీకరించక\",\"max_attention_chars\":\"aamktnvcduses\"},\n",
        "    {\"src_word\":\"amgiikarimchaadu\",\"pred_word\":\"అంగీకరించాడు\",\"ref_word\":\"అంగీకరించాడు\",\"max_attention_chars\":\"&lt;sos&gt;amktnvcduseyll\"},\n",
        "    {\"src_word\":\"angeekarinchaadu\",\"pred_word\":\"అంగీకరించాడు\",\"ref_word\":\"అంగీకరించాడు\",\"max_attention_chars\":\"aamktnvcduseyll\"},\n",
        "]\n",
        "\n",
        "# Preprocess: Remove <sos> tags and HTML-escaped chars (if required)\n",
        "for d in visualization_data:\n",
        "    d['max_attention_chars'] = d['max_attention_chars'].replace('&lt;sos&gt;', '').replace('&lt;', '<').replace('&gt;', '>')\n",
        "\n",
        "# Combine logic with line break after 25 characters per line\n",
        "max_chars_per_line = 25\n",
        "font_path = \"/content/NotoSansTelugu-VariableFont.ttf\"\n",
        "font_size = 40\n",
        "line_height = font_size + 10\n",
        "\n",
        "# lines: list of dicts with char and max_attention chars line wise\n",
        "lines = []\n",
        "current_line_chars = []\n",
        "current_line_attention = []\n",
        "\n",
        "for d in visualization_data:\n",
        "    tgt_word = d['pred_word']\n",
        "    attention_word = d['max_attention_chars']\n",
        "\n",
        "    for i, ch in enumerate(tgt_word):\n",
        "        att_char = attention_word[i] if i < len(attention_word) else ''\n",
        "        current_line_chars.append(ch)\n",
        "        current_line_attention.append(att_char)\n",
        "\n",
        "        if len(current_line_chars) == max_chars_per_line:\n",
        "            lines.append((current_line_chars, current_line_attention))\n",
        "            current_line_chars = []\n",
        "            current_line_attention = []\n",
        "\n",
        "# Add the last line if any chars remain\n",
        "if current_line_chars:\n",
        "    lines.append((current_line_chars, current_line_attention))\n",
        "\n",
        "# Calculate image height\n",
        "image_width = 1200\n",
        "image_height = line_height * len(lines) + 20\n",
        "\n",
        "# Create image and draw text line by line\n",
        "font = ImageFont.truetype(font_path, font_size)\n",
        "image = Image.new(\"RGB\", (image_width, image_height), \"white\")\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "char_positions = []\n",
        "y_offset = 10\n",
        "\n",
        "for line_idx, (chars, attns) in enumerate(lines):\n",
        "    current_x = 10\n",
        "    y = y_offset + line_idx * line_height\n",
        "\n",
        "    for idx, ch in enumerate(chars):\n",
        "        att_char = attns[idx] if idx < len(attns) else ''\n",
        "        char_bbox = draw.textbbox((current_x, y), ch, font=font)\n",
        "        char_width = char_bbox[2] - char_bbox[0]\n",
        "        char_height = char_bbox[3] - char_bbox[1]\n",
        "\n",
        "        draw.text((current_x, y), ch, font=font, fill=\"black\")\n",
        "\n",
        "        char_positions.append({\n",
        "            \"char\": ch,\n",
        "            \"x_min\": current_x,\n",
        "            \"x_max\": current_x + char_width,\n",
        "            \"y_min\": y,\n",
        "            \"y_max\": y + char_height,\n",
        "            \"max_attention\": att_char\n",
        "        })\n",
        "\n",
        "        current_x += char_width\n",
        "\n",
        "# Save image and encode\n",
        "image_path = \"combined_telugu_sentence_multiline.png\"\n",
        "image.save(image_path)\n",
        "with open(image_path, \"rb\") as f:\n",
        "    image_base64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "# Generate HTML\n",
        "html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <style>\n",
        "        #canvas {{ position: relative; border: 1px solid #ccc; }}\n",
        "        #tooltip {{\n",
        "            position: absolute;\n",
        "            background: rgba(0,0,0,0.8);\n",
        "            color: white;\n",
        "            padding: 5px 10px;\n",
        "            border-radius: 4px;\n",
        "            font-family: 'Noto Sans Telugu', Arial, sans-serif;\n",
        "            font-size: 14px;\n",
        "            pointer-events: none;\n",
        "            display: none;\n",
        "            z-index: 1000;\n",
        "            white-space: nowrap;\n",
        "        }}\n",
        "    </style>\n",
        "    <link href=\"https://fonts.googleapis.com/css2?family=Noto+Sans+Telugu&display=swap\" rel=\"stylesheet\">\n",
        "</head>\n",
        "<body>\n",
        "    <canvas id=\"canvas\" width=\"{image_width}\" height=\"{image_height}\"></canvas>\n",
        "    <div id=\"tooltip\"></div>\n",
        "    <script>\n",
        "        (function() {{\n",
        "            const canvas = document.getElementById('canvas');\n",
        "            const ctx = canvas.getContext('2d');\n",
        "            const img = new Image();\n",
        "            img.src = 'data:image/png;base64,{image_base64}';\n",
        "            img.onload = function() {{\n",
        "                ctx.drawImage(img, 0, 0);\n",
        "            }};\n",
        "\n",
        "            const regions = {json.dumps(char_positions)};\n",
        "            const tooltip = document.getElementById('tooltip');\n",
        "\n",
        "            function escapeHtml(text) {{\n",
        "                if (!text) return '';\n",
        "                return text.replace(/[&<>\"']/g, function(m) {{\n",
        "                    return {{'&':'&amp;','<':'&lt;','>':'&gt;','\"':'&quot;',\"'\":'&#39;'}}[m];\n",
        "                }});\n",
        "            }}\n",
        "\n",
        "            canvas.addEventListener('mousemove', function(e) {{\n",
        "                const rect = canvas.getBoundingClientRect();\n",
        "                const x = e.clientX - rect.left;\n",
        "                const y = e.clientY - rect.top;\n",
        "\n",
        "                let found = false;\n",
        "                for (let region of regions) {{\n",
        "                    if (x >= region.x_min && x <= region.x_max &&\n",
        "                        y >= region.y_min && y <= region.y_max) {{\n",
        "                            tooltip.style.display = 'block';\n",
        "                            tooltip.style.left = (e.clientX + 10) + 'px';\n",
        "                            tooltip.style.top = (e.clientY + 10) + 'px';\n",
        "\n",
        "                            const maxAtt = region.max_attention ? escapeHtml(region.max_attention) : '(none)';\n",
        "                            tooltip.innerHTML = 'Telugu Character: ' + escapeHtml(region.char) + '<br>Max Attention Char: ' + maxAtt;\n",
        "                            found = true;\n",
        "                            break;\n",
        "                    }}\n",
        "                }}\n",
        "\n",
        "                if (!found) {{\n",
        "                    tooltip.style.display = 'none';\n",
        "                }}\n",
        "            }});\n",
        "\n",
        "            canvas.addEventListener('mouseout', function() {{\n",
        "                tooltip.style.display = 'none';\n",
        "            }});\n",
        "        }})();\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Log to W&B\n",
        "wandb.log({\n",
        "    \"combined_telugu_attention_visualization_multiline\": wandb.Html(html_content)\n",
        "})\n",
        "\n",
        "try:\n",
        "    os.remove(image_path)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "run.finish()\n"
      ],
      "metadata": {
        "id": "Uk3_F2jiRV8W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}