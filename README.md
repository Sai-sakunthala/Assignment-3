# Assignment-3
This repository contains implementations and experiments for transliterating text from Latin script to Telugu script using sequence-to-sequence (seq2seq) neural network models. The goal is to learn mappings from Latin characters to Telugu characters, useful for applications like cross-script typing, language learning, or machine translation aids.

The models are implemented and trained in PyTorch, with training and evaluation carried out using Google Colab notebooks.

## Repository Contents and Descriptions

### Notebooks

- **`Vanilla_latin_to_telugu.ipynb`**  
  Implements a basic seq2seq encoder-decoder model **without attention**.  
  - Encoder processes the input Latin characters into a context vector.  
  - Decoder generates Telugu characters step-by-step from the context.  
  - Includes training loops, loss calculation, and basic evaluation.

- **`Vanilla_test.ipynb`**  
  Evaluates the vanilla seq2seq model on test data.  
  - Loads trained model checkpoint.  
  - Generates transliterations for unseen samples.  
  - Calculates word accuracy and character accuracy metrics.

- **`Attention_latin_to_telugu.ipynb`**  
  Implements a seq2seq model with an **attention mechanism**.  
  - Attention helps the decoder focus on relevant input parts for each output step.  
  - Usually improves transliteration quality for longer or complex inputs.  
  - Contains the full training pipeline with attention integration.

- **`Attention_test.ipynb`**  
  Evaluates the attention-based model.  
  - Loads attention model weights.  
  - Runs prediction on test data and computes accuracy and error metrics.


### Prediction Files

- **`prediction vanilla`**  
  Contains predicted Telugu transliterations generated by the vanilla seq2seq model on the test set.  
  Useful for manual inspection and comparison.

- **`predictions attention`**  
  Contains predicted outputs from the attention-based model on the test set.  
  Enables qualitative comparison with the vanilla model.

## Interactive_plot_code
This folder contains .ipynb and .txt files
### Files

- **`Interactive_visualization_code.ipynb`**  
  A Jupyter Notebook that:
  - Loads attention-related or positional data.
  - Renders a visualization

- **`interactivehtml.txt`**  
  A plain text file containing the full HTML + JavaScript code for the interactive plot.  
  **To use it:**
copy the content of .txt file and paste it after

html_content = f

line in the code Interactive_visualization_code.ipynb and run it

## Additional Details

- **Data:**  
  Uses a subset of the Dakshina dataset with parallel Latin-Telugu pairs.  
  Data preprocessing includes sequence length limiting, vocabulary creation, and character indexing.

- **Model Architecture:**  
  Both models use embeddings followed by RNN/GRU/LSTM layers.  
  The attention model adds an attention layer to improve decoding focus.

- **Training:**  
  Teacher forcing is used during training for faster convergence.  
  Metrics like word accuracy, character accuracy, and character error rate (CER) are tracked with Weights & Biases (wandb).

- **Evaluation:**  
  Evaluation notebooks load trained models and generate predictions on test sets, reporting accuracy and error metrics.
## How to Use This Repository

1. Open the Colab notebook of the model to train (`Vanilla_latin_to_telugu.ipynb` or `Attention_latin_to_telugu.ipynb`).  
2. Run the notebook to train the model with your configuration.  
3. After training, open the corresponding test notebook (`Vanilla_test.ipynb` or `Attention_test.ipynb`).  
4. Run evaluation cells to generate predictions and view metrics.  
5. Compare the prediction files (`prediction vanilla` vs `predictions attention`) for qualitative results.

## Acknowledgements

- Transliteration data sourced from the [Dakshina project](https://github.com/google-research-datasets/dakshina).  
- Model designs inspired by common seq2seq and attention mechanisms in NLP.
