{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11ts7jV4PejkBJrUOYj1Tj0HX6QQ8zXtR",
      "authorship_tag": "ABX9TyOysXnxN/VVFZjhoJBYQh8u"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hta9vgP-WmaL",
        "outputId": "4c094383-35b2-4860-811d-954cbed40c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install torch wandb pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import wandb\n",
        "import editdistance\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "LANG = 'te'\n",
        "data_path = f'/content/drive/MyDrive/dakshina_dataset_v1.0/{LANG}/lexicons/'\n",
        "\n",
        "def read_data(filepath, max_len=40):\n",
        "    pairs = []\n",
        "    with open(filepath, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            devanagiri, latin = parts[0], parts[1]  # Current order\n",
        "            # Swap to make Latin → Devanagiri\n",
        "            source, target = latin, devanagiri  # Now Latin is source, Devanagiri is target\n",
        "            if len(source) <= max_len and len(target) <= max_len:\n",
        "                pairs.append((source, target))\n",
        "    return pairs\n",
        "\n",
        "def make_vocab(sequences):\n",
        "    vocab = {'<pad>':0, '<sos>':1, '<eos>':2}\n",
        "    idx = 3\n",
        "    for seq in sequences:\n",
        "        for ch in seq:\n",
        "            if ch not in vocab:\n",
        "                vocab[ch] = idx\n",
        "                idx += 1\n",
        "    idx2char = {i:c for c,i in vocab.items()}\n",
        "    return vocab, idx2char\n",
        "\n",
        "def encode_word(word, vocab):\n",
        "    return [vocab['<sos>']] + [vocab[ch] for ch in word] + [vocab['<eos>']]\n",
        "\n",
        "def pad_seq(seq, max_len, pad_idx=0):\n",
        "    return seq + [pad_idx] * (max_len - len(seq))\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, pairs, source_vocab, target_vocab):\n",
        "        self.source_pad = source_vocab['<pad>']\n",
        "        self.target_pad = target_vocab['<pad>']\n",
        "        self.data = []\n",
        "        for source, target in pairs:\n",
        "            source_t = encode_word(source, source_vocab)\n",
        "            target_t = encode_word(target, target_vocab)\n",
        "            self.data.append((source_t, target_t))\n",
        "        self.source_max = max(len(x[0]) for x in self.data)\n",
        "        self.target_max = max(len(x[1]) for x in self.data)\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source, target = self.data[idx]\n",
        "        source = pad_seq(source, self.source_max, self.source_pad)\n",
        "        target = pad_seq(target, self.target_max, self.target_pad)\n",
        "        return torch.tensor(source), torch.tensor(target)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dimensions):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hid_dimensions * 2, hid_dimensions)\n",
        "        self.v = nn.Parameter(torch.rand(hid_dimensions))\n",
        "        stdv = 1. / (hid_dimensions ** 0.5)\n",
        "        self.v.data.uniform_(-stdv, stdv)\n",
        "        self.hid_dimensions = hid_dimensions\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: (batch_size, hid_dimensions) or (num_layers, batch_size, hid_dimensions)\n",
        "        # encoder_outputs: (batch_size, src_len, hid_dimensions)\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        src_len = encoder_outputs.size(1)\n",
        "\n",
        "        # Ensure hidden is 2D (batch_size, hid_dimensions)\n",
        "        if hidden.dim() == 3:  # (num_layers, batch_size, hid_dimensions)\n",
        "            hidden = hidden[-1]  # Take last layer: (batch_size, hid_dimensions)\n",
        "        elif hidden.dim() != 2:\n",
        "            raise ValueError(f\"Expected hidden to be 2D or 3D, got shape {hidden.shape}\")\n",
        "\n",
        "        # Repeat hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # (batch_size, src_len, hid_dimensions)\n",
        "\n",
        "        # Compute energy\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        energy = energy @ self.v  # (batch_size, src_len)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attn_weights = torch.softmax(energy, dim=1).unsqueeze(2)  # (batch_size, src_len, 1)\n",
        "\n",
        "        # Compute context vector\n",
        "        context = torch.sum(attn_weights * encoder_outputs, dim=1)  # (batch_size, hid_dimensions)\n",
        "\n",
        "        return context, attn_weights.squeeze(2)  # Return (batch_size, hid_dimensions), (batch_size, src_len)\n",
        "\n",
        "# Updated translit_Decoder to return attention weights\n",
        "class translit_Decoder(nn.Module):\n",
        "    def __init__(self, output_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell='lstm'):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dimensions, emb_dimensions)\n",
        "        self.attention = Attention(hid_dimensions)\n",
        "        rnn_cls = {'rnn': nn.RNN, 'gru': nn.GRU, 'lstm': nn.LSTM}[cell.lower()]\n",
        "        self.rnn = rnn_cls(emb_dimensions, hid_dimensions, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hid_dimensions * 2, output_dimensions)\n",
        "        self.cell = cell.lower()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "\n",
        "        input = input.unsqueeze(1)  # (batch_size, 1)\n",
        "        embedded = self.dropout(self.embedding(input))  # (batch_size, 1, emb_dimensions)\n",
        "\n",
        "        if self.cell == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        else:\n",
        "            output, hidden = self.rnn(embedded, hidden)\n",
        "            cell = None\n",
        "        context, attn_weights = self.attention(hidden, encoder_outputs)  # context: (batch_size, hid_dimensions), attn_weights: (batch_size, src_len)\n",
        "\n",
        "        rnn_output = output.squeeze(1)  # (batch_size, hid_dimensions)\n",
        "        combined = torch.cat((rnn_output, context), dim=1)  # (batch_size, hid_dimensions * 2)\n",
        "\n",
        "        prediction = self.fc_out(combined)  # (batch_size, output_dimensions)\n",
        "\n",
        "        return prediction, hidden, cell, attn_weights  # Added attn_weights\n",
        "\n",
        "class translit_Encoder(nn.Module):\n",
        "    def __init__(self, input_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell='lstm'):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dimensions, emb_dimensions)\n",
        "        rnn_cls = {'rnn': nn.RNN, 'gru': nn.GRU, 'lstm': nn.LSTM}[cell.lower()]\n",
        "        self.rnn = rnn_cls(emb_dimensions, hid_dimensions, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        self.attention = Attention(hid_dimensions)\n",
        "        self.cell = cell.lower()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, source):\n",
        "        embedded = self.dropout(self.embedding(source))\n",
        "        if self.cell == 'lstm':\n",
        "            outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        else:\n",
        "            outputs, hidden = self.rnn(embedded)\n",
        "            cell = None\n",
        "        context = self.attention(hidden, outputs)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class translit_Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.size(0)\n",
        "        target_len = target.size(1)\n",
        "        output_dimensions = self.decoder.fc_out.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, target_len, output_dimensions).to(self.device)\n",
        "        attn_weights_all = torch.zeros(batch_size, target_len, source.size(1)).to(self.device)  # To store attention weights\n",
        "\n",
        "        # Encoder\n",
        "        encoder_outputs, hidden, cell = self.encoder(source)\n",
        "\n",
        "        # First input\n",
        "        input = target[:, 0]\n",
        "\n",
        "        # Decoder loop\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell, attn_weights = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "            outputs[:, t] = output\n",
        "            attn_weights_all[:, t] = attn_weights  # Store attention weights\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = target[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs, attn_weights_all  # Return attention weights\n",
        "\n",
        "def strip_after_eos(seq, eos_idx):\n",
        "    if isinstance(seq, torch.Tensor):  # Handle tensors\n",
        "        seq = seq.cpu().numpy().tolist()\n",
        "    if eos_idx in seq:\n",
        "        return seq[:seq.index(eos_idx)]  # Exclude EOS for fair comparison\n",
        "    return seq\n",
        "\n",
        "def calculate_word_accuracy(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    correct = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        pred = strip_after_eos(pred, eos_idx) if eos_idx else pred\n",
        "        target = strip_after_eos(target, eos_idx) if eos_idx else target\n",
        "        pred = [p for p in pred if p != pad_idx]\n",
        "        target = [t for t in target if t != pad_idx]\n",
        "        correct += int(pred == target)\n",
        "    return correct / max(len(preds), 1)\n",
        "\n",
        "def calculate_cer(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    cer, total = 0, 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        pred = strip_after_eos(pred, eos_idx) if eos_idx else pred\n",
        "        target = strip_after_eos(target, eos_idx) if eos_idx else target\n",
        "        pred = [p for p in pred if p != pad_idx]\n",
        "        target = [t for t in target if t != pad_idx]\n",
        "        cer += editdistance.eval(pred, target)\n",
        "        total += max(len(target), 1)\n",
        "    return cer / total if total > 0 else float('inf')\n",
        "\n",
        "def calculate_accuracy(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        if isinstance(pred, torch.Tensor):\n",
        "            pred = pred.cpu().tolist()\n",
        "        if isinstance(target, torch.Tensor):\n",
        "            target = target.cpu().tolist()\n",
        "        if eos_idx is not None:\n",
        "            pred = strip_after_eos(pred, eos_idx)\n",
        "            target = strip_after_eos(target, eos_idx)\n",
        "        for p_token, t_token in zip(pred, target):\n",
        "            if t_token == pad_idx:\n",
        "                continue\n",
        "            if p_token == t_token:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return correct / total if total > 0 else 0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqP_qshcWqfD",
        "outputId": "d4984ec6-f5b4-46c8-9fdf-ed5838876695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "3g9F1-ANrbbl",
        "outputId": "be18c94e-fb47-4c9f-c5f5-62c3ee5860c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msai-sakunthala\u001b[0m (\u001b[33msai-sakunthala-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "\n",
        "# Verify font file (assuming you uploaded NotoSansTelugu-Regular.ttf)\n",
        "!ls /content\n",
        "\n",
        "# If the font is not in /content, specify the correct path where you uploaded it\n",
        "font_path = '/content/NotoSansTelugu-VariableFont.ttf'  # Adjust if uploaded to a different directory\n",
        "import os\n",
        "if os.path.exists(font_path):\n",
        "    print(f\"Font file found at {font_path}\")\n",
        "else:\n",
        "    print(f\"Font file not found at {font_path}. Please upload it.\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyFSj1ergZ6w",
        "outputId": "a44867f5-503b-4bd2-c41e-3cc5d84531d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "artifacts  heatmap_example_1.png   NotoSansTelugu-VariableFont.ttf  sample_data\n",
            "drive\t   LohitTeluguRegular.ttf  predictions_vanilla\t\t    wandb\n",
            "Font file found at /content/NotoSansTelugu-VariableFont.ttf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal test for create_heatmap_image\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "def create_heatmap_image(src_tokens, pred_tokens, attn_weights, idx, idx2char_src, idx2char_tgt):\n",
        "    \"\"\"Create a single heatmap image for WandB table using PIL\"\"\"\n",
        "    # Define image dimensions\n",
        "    cell_size = 50  # Size of each heatmap cell in pixels\n",
        "    label_width = 150  # Width for labels (source and target)\n",
        "    margin = 50  # Margin around the heatmap\n",
        "    title_height = 50  # Space for title\n",
        "    xlabel_height = 50  # Space for x-axis label\n",
        "    ylabel_width = 100  # Space for y-axis label\n",
        "\n",
        "    # Filter tokens and get labels\n",
        "    src_labels = [idx2char_src.get(idx, '?') for idx in src_tokens if idx in idx2char_src and idx not in [source_vocab.get('<pad>', -1), source_vocab.get('<sos>', -1), source_vocab.get('<eos>', -1)]]\n",
        "    pred_labels = [idx2char_tgt.get(idx, '?') for idx in pred_tokens if idx in idx2char_tgt and idx not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1), target_vocab.get('<eos>', -1)]]\n",
        "\n",
        "    # Debug labels with Unicode code points\n",
        "    print(f\"Example {idx+1} - Source tokens: {src_tokens}\")\n",
        "    print(f\"Example {idx+1} - Source labels: {src_labels}\")\n",
        "    print(f\"Example {idx+1} - Pred tokens: {pred_tokens}\")\n",
        "    print(f\"Example {idx+1} - Pred labels: {pred_labels}\")\n",
        "    print(f\"Example {idx+1} - Pred labels (Unicode): {[f'U+{ord(c):04X}' for c in pred_labels if c != '?']}\")\n",
        "    print(f\"Example {idx+1} - Attention weights shape: {attn_weights.shape}\")\n",
        "\n",
        "    # Filter out invalid characters (non-Telugu or '?')\n",
        "    valid_pred_labels = []\n",
        "    for char in pred_labels:\n",
        "        if char == '?' or not (0x0C00 <= ord(char) <= 0x0C7F):  # Telugu Unicode range\n",
        "            valid_pred_labels.append('?')  # Replace invalid chars with '?'\n",
        "        else:\n",
        "            valid_pred_labels.append(char)\n",
        "    pred_labels = valid_pred_labels\n",
        "    print(f\"Example {idx+1} - Filtered pred_labels: {pred_labels}\")\n",
        "\n",
        "    # Truncate attention weights to match label lengths\n",
        "    attn_weights = attn_weights[:min(len(pred_labels), attn_weights.shape[0]),\n",
        "                                :min(len(src_labels), attn_weights.shape[1])]\n",
        "    print(f\"Example {idx+1} - Truncated attention weights shape: {attn_weights.shape}\")\n",
        "\n",
        "    # Calculate image size\n",
        "    heatmap_width = len(src_labels) * cell_size\n",
        "    heatmap_height = len(pred_labels) * cell_size\n",
        "    img_width = heatmap_width + label_width + ylabel_width + 2 * margin\n",
        "    img_height = heatmap_height + label_width + title_height + xlabel_height + 2 * margin\n",
        "\n",
        "    # Create a new image with white background\n",
        "    image = Image.new('RGB', (img_width, img_height), 'white')\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Load Telugu font\n",
        "    font_path = '/content/LohitTeluguRegular.ttf'\n",
        "    if not os.path.exists(font_path):\n",
        "        raise FileNotFoundError(f\"Telugu font not found at {font_path}.\")\n",
        "    try:\n",
        "        telugu_font = ImageFont.truetype(font_path, size=20)\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to load font {font_path}: {e}\")\n",
        "\n",
        "    # Default font for Latin text (use FreeSans if available, or fallback to default)\n",
        "    try:\n",
        "        latin_font = ImageFont.truetype('/usr/share/fonts/truetype/freefont/FreeSans.ttf', size=20)\n",
        "    except:\n",
        "        latin_font = ImageFont.load_default()\n",
        "\n",
        "    # Draw title\n",
        "    title = f'Example {idx+1}'\n",
        "    draw.text((margin + ylabel_width, margin), title, font=latin_font, fill='black')\n",
        "\n",
        "    # Draw x-axis labels (Source Tokens - Latin)\n",
        "    for i, label in enumerate(src_labels):\n",
        "        x = margin + ylabel_width + i * cell_size + cell_size // 2\n",
        "        y = margin + title_height + heatmap_height + 10\n",
        "        draw.text((x, y), label, font=latin_font, fill='black', anchor='mm')\n",
        "\n",
        "    # Draw y-axis labels (Target Tokens - Telugu)\n",
        "    for i, label in enumerate(pred_labels):\n",
        "        x = margin + ylabel_width - 10\n",
        "        y = margin + title_height + i * cell_size + cell_size // 2\n",
        "        draw.text((x, y), label, font=telugu_font, fill='black', anchor='rm')\n",
        "\n",
        "    # Draw x-axis title\n",
        "    draw.text((margin + ylabel_width + heatmap_width // 2, margin + title_height + heatmap_height + xlabel_height - 10),\n",
        "              'Source Tokens (Latin)', font=latin_font, fill='black', anchor='mm')\n",
        "\n",
        "    # Draw y-axis title\n",
        "    draw.text((margin + ylabel_width // 2, margin + title_height + heatmap_height // 2),\n",
        "              'Target Tokens (Telugu)', font=telugu_font, fill='black', angle=90, anchor='mm')\n",
        "\n",
        "    # Draw heatmap\n",
        "    for i in range(len(pred_labels)):\n",
        "        for j in range(len(src_labels)):\n",
        "            # Normalize attention weights to [0, 1] for color mapping\n",
        "            weight = attn_weights[i, j]\n",
        "            # Map to a color (viridis-like: 0=blue, 1=yellow)\n",
        "            r = int(255 * weight)\n",
        "            g = int(255 * (1 - weight))\n",
        "            b = 0\n",
        "            color = (r, g, b)\n",
        "            x0 = margin + ylabel_width + j * cell_size\n",
        "            y0 = margin + title_height + i * cell_size\n",
        "            draw.rectangle([x0, y0, x0 + cell_size, y0 + cell_size], fill=color)\n",
        "\n",
        "    # Save image for verification\n",
        "    if idx == 0:\n",
        "        image.save(f\"/content/heatmap_example_{idx+1}.png\")\n",
        "        print(f\"Saved sample heatmap to /content/heatmap_example_{idx+1}.png\")\n",
        "\n",
        "    # Convert to WandB image\n",
        "    wandb_image = wandb.Image(image, caption=f\"Attention Heatmap Example {idx+1}\")\n",
        "    return wandb_image"
      ],
      "metadata": {
        "id": "M9sqCVD8lDi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "run = wandb.init(project=\"dakshina-seq2seq\", entity=\"sai-sakunthala-indian-institute-of-technology-madras\", name=\"evaluate_test\")\n",
        "artifact = run.use_artifact('sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq-3/best_model:v52', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "# Read data and create vocabularies\n",
        "test_pairs = read_data(data_path + f\"{LANG}.translit.sampled.test.tsv\", max_len=30)\n",
        "train_pairs = read_data(data_path + f\"{LANG}.translit.sampled.train.tsv\", max_len=30)\n",
        "source_vocab, idx2char_src = make_vocab([x[0] for x in train_pairs])\n",
        "target_vocab, idx2char_tgt = make_vocab([x[1] for x in train_pairs])\n",
        "\n",
        "# Model parameters (must match training)\n",
        "input_dimensions = len(source_vocab)\n",
        "output_dimensions = len(target_vocab)\n",
        "emb_dimensions = 128\n",
        "hid_dimensions = 128 * 2\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "cell = 'lstm'\n",
        "batch_size = 64\n",
        "max_len = 30\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize model\n",
        "encoder = translit_Encoder(input_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell).to(device)\n",
        "decoder = translit_Decoder(output_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell).to(device)\n",
        "model = translit_Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "# Load model weights\n",
        "state_dict = torch.load(f\"{artifact_dir}/best_model.pt\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Create test dataset and loader\n",
        "test_translit = TransliterationDataset(test_pairs, source_vocab, target_vocab)\n",
        "test_loader = DataLoader(test_translit, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "all_src, all_preds, all_tgts, all_attn_weights = [], [], [], []\n",
        "correct = 0\n",
        "total = 0\n",
        "selected_examples = []\n",
        "\n",
        "def predict(model, src, max_len=30):\n",
        "    \"\"\"Greedy decoding implementation with attention weights\"\"\"\n",
        "    encoder_outputs, encoder_hidden, encoder_cell = model.encoder(src)\n",
        "    input = torch.tensor([target_vocab['<sos>']] * src.size(0)).to(device)\n",
        "    outputs = []\n",
        "    attn_weights_list = []\n",
        "\n",
        "    for t in range(max_len):\n",
        "        output, encoder_hidden, encoder_cell, attn_weights = model.decoder(input, encoder_hidden, encoder_cell, encoder_outputs)\n",
        "        input = output.argmax(1)  # Greedy decoding\n",
        "        outputs.append(input)\n",
        "        attn_weights_list.append(attn_weights)  # Collect attention weights\n",
        "\n",
        "        # Stop if all sequences predicted EOS\n",
        "        if (input == target_vocab.get('<eos>', -1)).all():\n",
        "            break\n",
        "\n",
        "    outputs = torch.stack(outputs, dim=1)  # (batch_size, max_len)\n",
        "    attn_weights_all = torch.stack(attn_weights_list, dim=1)  # (batch_size, max_len, src_len)\n",
        "    return outputs, attn_weights_all\n",
        "\n",
        "def plot_attention_heatmap_grid(sources, predictions, attn_weights_list, idx2char_src, idx2char_tgt, num_plots=12):\n",
        "    \"\"\"Create a 4x3 grid of attention heatmaps in a WandB table\"\"\"\n",
        "    table = wandb.Table(columns=['Heatmap 1', 'Heatmap 2', 'Heatmap 3'])\n",
        "\n",
        "    # Generate heatmaps for each example\n",
        "    heatmap_images = []\n",
        "    for i in range(min(num_plots, len(sources))):\n",
        "        heatmap_images.append(create_heatmap_image(\n",
        "            sources[i], predictions[i], attn_weights_list[i], i, idx2char_src, idx2char_tgt\n",
        "        ))\n",
        "\n",
        "    # Fill table rows (4 rows, 3 columns each)\n",
        "    for row_idx in range(4):\n",
        "        row_data = [heatmap_images[row_idx * 3 + col_idx] if row_idx * 3 + col_idx < len(heatmap_images) else None for col_idx in range(3)]\n",
        "        table.add_data(*row_data)\n",
        "\n",
        "    wandb.log({\"Attention Heatmap Grid (4x3)\": table})\n",
        "\n",
        "def log_attention_heatmaps_individually(sources, predictions, attn_weights_list, idx2char_src, idx2char_tgt, num_plots=10):\n",
        "    for i in range(min(num_plots, len(sources))):\n",
        "        heatmap_img = create_heatmap_image(\n",
        "            sources[i], predictions[i], attn_weights_list[i], i, idx2char_src, idx2char_tgt\n",
        "        )\n",
        "        wandb.log({f\"Attention Heatmap {i+1}\": wandb.Image(heatmap_img)})\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, tgt in tqdm(test_loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        preds, attn_weights_batch = predict(model, src)\n",
        "\n",
        "        # Convert to numpy arrays for processing\n",
        "        src_np = src.cpu().numpy()\n",
        "        preds_np = preds.cpu().numpy()\n",
        "        tgt_np = tgt.cpu().numpy()\n",
        "        attn_weights_np = attn_weights_batch.cpu().numpy()  # (batch_size, max_len, src_len)\n",
        "\n",
        "        for i in range(len(src_np)):\n",
        "            # Get source, prediction, target, and attention weights\n",
        "            s = src_np[i]\n",
        "            p = preds_np[i]\n",
        "            t = tgt_np[i]\n",
        "            attn = attn_weights_np[i]\n",
        "\n",
        "            # Store sequences and attention weights for all examples\n",
        "            all_src.append(s)\n",
        "            all_preds.append(p)\n",
        "            all_tgts.append(t)\n",
        "            all_attn_weights.append(attn)\n",
        "\n",
        "            # Collect up to 12 examples for heatmaps\n",
        "            if len(selected_examples) < 12:\n",
        "                selected_examples.append((s, p, t, attn))\n",
        "\n",
        "            # Process prediction: remove padding and everything after EOS\n",
        "            p_processed = []\n",
        "            for token in p:\n",
        "                if token == target_vocab.get('<eos>', -1):\n",
        "                    break\n",
        "                if token not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1)]:\n",
        "                    p_processed.append(token)\n",
        "\n",
        "            # Process target: remove padding and everything after EOS\n",
        "            t_processed = []\n",
        "            for token in t:\n",
        "                if token == target_vocab.get('<eos>', -1):\n",
        "                    break\n",
        "                if token not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1)]:\n",
        "                    t_processed.append(token)\n",
        "\n",
        "            # Compare the processed sequences\n",
        "            if p_processed == t_processed:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "# Plot and log attention heatmaps for 12 examples\n",
        "if len(selected_examples) >= 10:\n",
        "    sources, predictions, _, attn_weights_list = zip(*selected_examples[:12])\n",
        "    log_attention_heatmaps_individually(sources, predictions, attn_weights_list, idx2char_src, idx2char_tgt, num_plots=12)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct / total if total > 0 else 0\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Correct: {correct}, Total: {total}\")\n",
        "wandb.log({\"Test Accuracy\": accuracy})\n",
        "\n",
        "# Log sample predictions table with at least 7 correct labels\n",
        "def log_table_wandb(sources, preds, targets, idx2char_src, idx2char_tgt, num_samples=10, min_correct=7):\n",
        "    table = wandb.Table(columns=[\"Source\", \"Prediction\", \"Reference\", \"Status\"])\n",
        "\n",
        "    # Collect correct and incorrect indices\n",
        "    correct_indices = []\n",
        "    incorrect_indices = []\n",
        "    for i in range(len(sources)):\n",
        "        src_word = ''.join([idx2char_src.get(idx, '?') for idx in sources[i] if idx not in [source_vocab.get('<pad>', -1), source_vocab.get('<sos>', -1), source_vocab.get('<eos>', -1)]])\n",
        "        pred_word = ''.join([idx2char_tgt.get(idx, '?') for idx in preds[i] if idx not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1), target_vocab.get('<eos>', -1)]])\n",
        "        ref_word = ''.join([idx2char_tgt.get(idx, '?') for idx in targets[i] if idx not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1), target_vocab.get('<eos>', -1)]])\n",
        "        is_correct = (pred_word == ref_word)\n",
        "        if is_correct:\n",
        "            correct_indices.append((i, src_word, pred_word, ref_word))\n",
        "        else:\n",
        "            incorrect_indices.append((i, src_word, pred_word, ref_word))\n",
        "\n",
        "    # Select at least min_correct correct samples, or all if fewer are available\n",
        "    num_correct = min(len(correct_indices), min_correct)\n",
        "    selected_correct = random.sample(correct_indices, num_correct) if correct_indices else []\n",
        "\n",
        "    # Fill remaining slots with incorrect samples, up to num_samples\n",
        "    remaining_slots = num_samples - len(selected_correct)\n",
        "    selected_incorrect = random.sample(incorrect_indices, min(remaining_slots, len(incorrect_indices))) if incorrect_indices and remaining_slots > 0 else []\n",
        "\n",
        "    # Combine and shuffle selected samples\n",
        "    selected_samples = selected_correct + selected_incorrect\n",
        "    random.shuffle(selected_samples)\n",
        "\n",
        "    # Add to table\n",
        "    for i, src_word, pred_word, ref_word in selected_samples:\n",
        "        is_correct = (pred_word == ref_word)\n",
        "        status = \"🟩 **Correct**\" if is_correct else \"🟥 **Incorrect**\"\n",
        "        table.add_data(src_word, pred_word, ref_word, status)\n",
        "\n",
        "    wandb.log({\"Test Sample Predictions (Color-Coded)\": table})\n",
        "    print(f\"Logging table: correct={len(correct_indices)}, incorrect={len(incorrect_indices)}\")\n",
        "\n",
        "log_table_wandb(all_src, all_preds, all_tgts, idx2char_src, idx2char_tgt, num_samples=10, min_correct=7)\n",
        "\n",
        "# Save predictions to file\n",
        "output_dir = \"predictions_vanilla\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(output_dir, \"test_predictions.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for s, p, t in zip(all_src, all_preds, all_tgts):\n",
        "        src_word = ''.join([idx2char_src.get(idx, '?') for idx in s if idx not in [source_vocab.get('<pad>', -1), source_vocab.get('<sos>', -1), source_vocab.get('<eos>', -1)]])\n",
        "        pred_word = ''.join([idx2char_tgt.get(idx, '?') for idx in p if idx not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1), target_vocab.get('<eos>', -1)]])\n",
        "        ref_word = ''.join([idx2char_tgt.get(idx, '?') for idx in t if idx not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1), target_vocab.get('<eos>', -1)]])\n",
        "        f.write(f\"{src_word}\\t{pred_word}\\t{ref_word}\\n\")\n",
        "\n",
        "print(f\"Saved full predictions to: {output_dir}/test_predictions.txt\")\n",
        "wandb.save(os.path.join(output_dir, \"test_predictions.txt\"))\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LJ4TX8mGsMzu",
        "outputId": "b26c24db-0ce4-4dd7-b1e3-dc1f0ce9e194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_121053-bvi269wn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq/runs/bvi269wn' target=\"_blank\">evaluate_test</a></strong> to <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq/runs/bvi269wn' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq/runs/bvi269wn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "100%|██████████| 89/89 [00:50<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 - Source tokens: [ 1  3  4  5  3  4 18 21  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0]\n",
            "Example 1 - Source labels: ['a', 'm', 'k', 'a', 'm', 'l', 'o']\n",
            "Example 1 - Pred tokens: [ 3  4  5  4 21 29  2  2  2  2  2  2  2  2 29  2  2  2 29  2  2  2 29  2\n",
            "  2  2 29  2  2  2]\n",
            "Example 1 - Pred labels: ['అ', 'ం', 'క', 'ం', 'ల', 'ో', 'ో', 'ో', 'ో', 'ో']\n",
            "Example 1 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C15', 'U+0C02', 'U+0C32', 'U+0C4B', 'U+0C4B', 'U+0C4B', 'U+0C4B', 'U+0C4B']\n",
            "Example 1 - Attention weights shape: (30, 25)\n",
            "Example 1 - Filtered pred_labels: ['అ', 'ం', 'క', 'ం', 'ల', 'ో', 'ో', 'ో', 'ో', 'ో']\n",
            "Example 1 - Truncated attention weights shape: (10, 7)\n",
            "Saved sample heatmap to /content/heatmap_example_1.png\n",
            "Example 2 - Source tokens: [ 1  3  8  5  3  4 18 21  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0]\n",
            "Example 2 - Source labels: ['a', 'n', 'k', 'a', 'm', 'l', 'o']\n",
            "Example 2 - Pred tokens: [ 3  4  5  4 21 29  2  2  2  2  2  2  2  2 29  2  2  2 29  2 29  2  2  2\n",
            "  2  2 29  2  2  2]\n",
            "Example 2 - Pred labels: ['అ', 'ం', 'క', 'ం', 'ల', 'ో', 'ో', 'ో', 'ో', 'ో']\n",
            "Example 2 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C15', 'U+0C02', 'U+0C32', 'U+0C4B', 'U+0C4B', 'U+0C4B', 'U+0C4B', 'U+0C4B']\n",
            "Example 2 - Attention weights shape: (30, 25)\n",
            "Example 2 - Filtered pred_labels: ['అ', 'ం', 'క', 'ం', 'ల', 'ో', 'ో', 'ో', 'ో', 'ో']\n",
            "Example 2 - Truncated attention weights shape: (10, 7)\n",
            "Example 3 - Source tokens: [ 1  3  8  5  3  4 18 21 21  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0]\n",
            "Example 3 - Source labels: ['a', 'n', 'k', 'a', 'm', 'l', 'o', 'o']\n",
            "Example 3 - Pred tokens: [ 3  4  5  4 21 29  2  2  2  2  2  2  2  2 29  2  2  2 29  2  2  2 29  2\n",
            "  2  2  2 29  2  2]\n",
            "Example 3 - Pred labels: ['అ', 'ం', 'క', 'ం', 'ల', 'ో', 'ో', 'ో', 'ో', 'ో']\n",
            "Example 3 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C15', 'U+0C02', 'U+0C32', 'U+0C4B', 'U+0C4B', 'U+0C4B', 'U+0C4B', 'U+0C4B']\n",
            "Example 3 - Attention weights shape: (30, 25)\n",
            "Example 3 - Filtered pred_labels: ['అ', 'ం', 'క', 'ం', 'ల', 'ో', 'ో', 'ో', 'ో', 'ో']\n",
            "Example 3 - Truncated attention weights shape: (10, 8)\n",
            "Example 4 - Source tokens: [1 3 4 5 6 7 3 4 3 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example 4 - Source labels: ['a', 'm', 'k', 'i', 't', 'a', 'm', 'a', 'i']\n",
            "Example 4 - Pred tokens: [ 3  4  5  6  7 11 17  2  2  2  2  2  2  2  2  2  2 17  2  2  2 17  2 17\n",
            "  2  2  2 17  2 17]\n",
            "Example 4 - Pred labels: ['అ', 'ం', 'క', 'ి', 'త', 'మ', 'ై', 'ై', 'ై', 'ై', 'ై', 'ై']\n",
            "Example 4 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C15', 'U+0C3F', 'U+0C24', 'U+0C2E', 'U+0C48', 'U+0C48', 'U+0C48', 'U+0C48', 'U+0C48', 'U+0C48']\n",
            "Example 4 - Attention weights shape: (30, 25)\n",
            "Example 4 - Filtered pred_labels: ['అ', 'ం', 'క', 'ి', 'త', 'మ', 'ై', 'ై', 'ై', 'ై', 'ై', 'ై']\n",
            "Example 4 - Truncated attention weights shape: (12, 9)\n",
            "Example 5 - Source tokens: [1 3 8 5 6 7 3 4 3 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example 5 - Source labels: ['a', 'n', 'k', 'i', 't', 'a', 'm', 'a', 'i']\n",
            "Example 5 - Pred tokens: [ 3  4  5  6  7 11 17  2  2  2  2  2  2  2  2  2  2  2 17  2  2  2 17  2\n",
            " 17  2  2  2 17  2]\n",
            "Example 5 - Pred labels: ['అ', 'ం', 'క', 'ి', 'త', 'మ', 'ై', 'ై', 'ై', 'ై', 'ై']\n",
            "Example 5 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C15', 'U+0C3F', 'U+0C24', 'U+0C2E', 'U+0C48', 'U+0C48', 'U+0C48', 'U+0C48', 'U+0C48']\n",
            "Example 5 - Attention weights shape: (30, 25)\n",
            "Example 5 - Filtered pred_labels: ['అ', 'ం', 'క', 'ి', 'త', 'మ', 'ై', 'ై', 'ై', 'ై', 'ై']\n",
            "Example 5 - Truncated attention weights shape: (11, 9)\n",
            "Example 6 - Source tokens: [ 1  3  8  5 17 18  3  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0]\n",
            "Example 6 - Source labels: ['a', 'n', 'k', 'e', 'l', 'a']\n",
            "Example 6 - Pred tokens: [ 3  4  5 20 21  2  2  2  2  2  2  2  2  2  2  2  2  2  2  9 21  2  2  2\n",
            "  2  2  9  2  2  2]\n",
            "Example 6 - Pred labels: ['అ', 'ం', 'క', 'ె', 'ల', 'ా', 'ల', 'ా']\n",
            "Example 6 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C15', 'U+0C46', 'U+0C32', 'U+0C3E', 'U+0C32', 'U+0C3E']\n",
            "Example 6 - Attention weights shape: (30, 25)\n",
            "Example 6 - Filtered pred_labels: ['అ', 'ం', 'క', 'ె', 'ల', 'ా', 'ల', 'ా']\n",
            "Example 6 - Truncated attention weights shape: (8, 6)\n",
            "Example 7 - Source tokens: [ 1  3  8  5 17 18  3  8 14  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0]\n",
            "Example 7 - Source labels: ['a', 'n', 'k', 'e', 'l', 'a', 'n', 'u']\n",
            "Example 7 - Pred tokens: [ 3  4  5 20 21 18 15  2  2  2  2 15  2 15  2 15  2 15  2 15 18 15  2 15\n",
            "  2 15  2 15 18 15]\n",
            "Example 7 - Pred labels: ['అ', 'ం', 'క', 'ె', 'ల', 'న', 'ు', 'ు', 'ు', 'ు', 'ు', 'ు', 'న', 'ు', 'ు', 'ు', 'ు', 'న', 'ు']\n",
            "Example 7 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C15', 'U+0C46', 'U+0C32', 'U+0C28', 'U+0C41', 'U+0C41', 'U+0C41', 'U+0C41', 'U+0C41', 'U+0C41', 'U+0C28', 'U+0C41', 'U+0C41', 'U+0C41', 'U+0C41', 'U+0C28', 'U+0C41']\n",
            "Example 7 - Attention weights shape: (30, 25)\n",
            "Example 7 - Filtered pred_labels: ['అ', 'ం', 'క', 'ె', 'ల', 'న', 'ు', 'ు', 'ు', 'ు', 'ు', 'ు', 'న', 'ు', 'ు', 'ు', 'ు', 'న', 'ు']\n",
            "Example 7 - Truncated attention weights shape: (19, 8)\n",
            "Example 8 - Source tokens: [ 1  3  8 20 17 17  5  3 15  6  8 12  9  3  5  3  2  0  0  0  0  0  0  0\n",
            "  0]\n",
            "Example 8 - Source labels: ['a', 'n', 'g', 'e', 'e', 'k', 'a', 'r', 'i', 'n', 'c', 'h', 'a', 'k', 'a']\n",
            "Example 8 - Pred tokens: [ 3  4 22 26  5 16  6  4 12  5  2  2  2  2  2  2  2  2  2  2  2  5  2  2\n",
            "  2  2  5  5  5  2]\n",
            "Example 8 - Pred labels: ['అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'క', 'క', 'క', 'క', 'క']\n",
            "Example 8 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C17', 'U+0C40', 'U+0C15', 'U+0C30', 'U+0C3F', 'U+0C02', 'U+0C1A', 'U+0C15', 'U+0C15', 'U+0C15', 'U+0C15', 'U+0C15']\n",
            "Example 8 - Attention weights shape: (30, 25)\n",
            "Example 8 - Filtered pred_labels: ['అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'క', 'క', 'క', 'క', 'క']\n",
            "Example 8 - Truncated attention weights shape: (14, 15)\n",
            "Example 9 - Source tokens: [ 1  3  4 20  6  6  5  3 15  6  4 12  9  3  3 13 14  2  0  0  0  0  0  0\n",
            "  0]\n",
            "Example 9 - Source labels: ['a', 'm', 'g', 'i', 'i', 'k', 'a', 'r', 'i', 'm', 'c', 'h', 'a', 'a', 'd', 'u']\n",
            "Example 9 - Pred tokens: [ 3  4 22 26  5 16  6  4 12  9 14 15  2  2  2  2  2  2 15  2  2  2 15  2\n",
            " 15  2 23 15  2 15]\n",
            "Example 9 - Pred labels: ['అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'ా', 'డ', 'ు', 'ు', 'ు', 'ు', 'ద', 'ు', 'ు']\n",
            "Example 9 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C17', 'U+0C40', 'U+0C15', 'U+0C30', 'U+0C3F', 'U+0C02', 'U+0C1A', 'U+0C3E', 'U+0C21', 'U+0C41', 'U+0C41', 'U+0C41', 'U+0C41', 'U+0C26', 'U+0C41', 'U+0C41']\n",
            "Example 9 - Attention weights shape: (30, 25)\n",
            "Example 9 - Filtered pred_labels: ['అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'ా', 'డ', 'ు', 'ు', 'ు', 'ు', 'ద', 'ు', 'ు']\n",
            "Example 9 - Truncated attention weights shape: (18, 16)\n",
            "Example 10 - Source tokens: [ 1  3  8 20 17 17  5  3 15  6  8 12  9  3  3 13 14  2  0  0  0  0  0  0\n",
            "  0]\n",
            "Example 10 - Source labels: ['a', 'n', 'g', 'e', 'e', 'k', 'a', 'r', 'i', 'n', 'c', 'h', 'a', 'a', 'd', 'u']\n",
            "Example 10 - Pred tokens: [ 3  4 22 26  5 16  6  4 12  9 14 15  2  2  2  2  2  2  2 15  2  2  2 15\n",
            "  2 23 15  2 15  2]\n",
            "Example 10 - Pred labels: ['అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'ా', 'డ', 'ు', 'ు', 'ు', 'ద', 'ు', 'ు']\n",
            "Example 10 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C17', 'U+0C40', 'U+0C15', 'U+0C30', 'U+0C3F', 'U+0C02', 'U+0C1A', 'U+0C3E', 'U+0C21', 'U+0C41', 'U+0C41', 'U+0C41', 'U+0C26', 'U+0C41', 'U+0C41']\n",
            "Example 10 - Attention weights shape: (30, 25)\n",
            "Example 10 - Filtered pred_labels: ['అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'ా', 'డ', 'ు', 'ు', 'ు', 'ద', 'ు', 'ు']\n",
            "Example 10 - Truncated attention weights shape: (17, 16)\n",
            "Example 11 - Source tokens: [ 1  3  8 20  6  6  5  3 15  6  8 12  9  3  3 13 14  2  0  0  0  0  0  0\n",
            "  0]\n",
            "Example 11 - Source labels: ['a', 'n', 'g', 'i', 'i', 'k', 'a', 'r', 'i', 'n', 'c', 'h', 'a', 'a', 'd', 'u']\n",
            "Example 11 - Pred tokens: [ 3  4 22 26  5 16  6  4 12  9 14 15  2  2  2  2  2  2 15  2  2  2 15  2\n",
            " 15  2 23 15  2 15]\n",
            "Example 11 - Pred labels: ['అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'ా', 'డ', 'ు', 'ు', 'ు', 'ు', 'ద', 'ు', 'ు']\n",
            "Example 11 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C17', 'U+0C40', 'U+0C15', 'U+0C30', 'U+0C3F', 'U+0C02', 'U+0C1A', 'U+0C3E', 'U+0C21', 'U+0C41', 'U+0C41', 'U+0C41', 'U+0C41', 'U+0C26', 'U+0C41', 'U+0C41']\n",
            "Example 11 - Attention weights shape: (30, 25)\n",
            "Example 11 - Filtered pred_labels: ['అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'ా', 'డ', 'ు', 'ు', 'ు', 'ు', 'ద', 'ు', 'ు']\n",
            "Example 11 - Truncated attention weights shape: (18, 16)\n",
            "Example 12 - Source tokens: [ 1  3  8 20 17 17  5  3 15  6  8 12  9  3  3 18  6  2  0  0  0  0  0  0\n",
            "  0]\n",
            "Example 12 - Source labels: ['a', 'n', 'g', 'e', 'e', 'k', 'a', 'r', 'i', 'n', 'c', 'h', 'a', 'a', 'l', 'i']\n",
            "Example 12 - Pred tokens: [ 3  4 22 26  5 16  6  4 12  9 21  6  2  6  2  6  2  6  2  6  2  6  2  6\n",
            "  2  6  2  6  2  6]\n",
            "Example 12 - Pred labels: ['అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'ా', 'ల', 'ి', 'ి', 'ి', 'ి', 'ి', 'ి', 'ి', 'ి', 'ి', 'ి']\n",
            "Example 12 - Pred labels (Unicode): ['U+0C05', 'U+0C02', 'U+0C17', 'U+0C40', 'U+0C15', 'U+0C30', 'U+0C3F', 'U+0C02', 'U+0C1A', 'U+0C3E', 'U+0C32', 'U+0C3F', 'U+0C3F', 'U+0C3F', 'U+0C3F', 'U+0C3F', 'U+0C3F', 'U+0C3F', 'U+0C3F', 'U+0C3F', 'U+0C3F']\n",
            "Example 12 - Attention weights shape: (30, 25)\n",
            "Example 12 - Filtered pred_labels: ['అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'ా', 'ల', 'ి', 'ి', 'ి', 'ి', 'ి', 'ి', 'ి', 'ి', 'ి', 'ి']\n",
            "Example 12 - Truncated attention weights shape: (21, 16)\n",
            "Test Accuracy: 0.5858\n",
            "Correct: 3337, Total: 5696\n",
            "Logging table: correct=164, incorrect=5532\n",
            "Saved full predictions to: predictions_vanilla/test_predictions.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.58585</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">evaluate_test</strong> at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq/runs/bvi269wn' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq/runs/bvi269wn</a><br> View project at: <a href='https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq' target=\"_blank\">https://wandb.ai/sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq</a><br>Synced 5 W&B file(s), 13 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_121053-bvi269wn/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}