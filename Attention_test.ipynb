{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11ts7jV4PejkBJrUOYj1Tj0HX6QQ8zXtR",
      "authorship_tag": "ABX9TyMZ1mYNsy+VWHTiXmBZwgnF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hta9vgP-WmaL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install torch wandb pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import wandb\n",
        "import editdistance\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "LANG = 'te'\n",
        "data_path = f'/content/drive/MyDrive/dakshina_dataset_v1.0/{LANG}/lexicons/'\n",
        "\n",
        "def read_data(filepath, max_len=40):\n",
        "    pairs = []\n",
        "    # Open the file with UTF-8 encoding to properly read Unicode characters\n",
        "    with open(filepath, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            # Remove leading/trailing whitespace and split by tab\n",
        "            parts = line.strip().split('\\t')\n",
        "            # Skip lines that don't contain both source and target text\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            devanagiri, latin = parts[0], parts[1]\n",
        "\n",
        "            # We are training a Latin → Devanagiri transliteration model,\n",
        "            # so set Latin as the source and Devanagiri as the target\n",
        "            source, target = latin, devanagiri\n",
        "\n",
        "            # Only keep pairs where both source and target are within the allowed max length\n",
        "            if len(source) <= max_len and len(target) <= max_len:\n",
        "                pairs.append((source, target))\n",
        "\n",
        "    # Return the list of filtered (source, target) pairs\n",
        "    return pairs\n",
        "\n",
        "def make_vocab(sequences):\n",
        "    # Initialize the vocabulary with special tokens\n",
        "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n",
        "    idx = 3  # Starting index for regular characters\n",
        "\n",
        "    # Loop through all sequences to build the vocabulary\n",
        "    for seq in sequences:\n",
        "        for ch in seq:\n",
        "            # Add each unique character to the vocabulary\n",
        "            if ch not in vocab:\n",
        "                vocab[ch] = idx\n",
        "                idx += 1\n",
        "\n",
        "    # Create reverse mapping from index to character\n",
        "    idx2char = {i: c for c, i in vocab.items()}\n",
        "\n",
        "    # Return both the character-to-index and index-to-character dictionaries\n",
        "    return vocab, idx2char\n",
        "\n",
        "def encode_word(word, vocab):\n",
        "    # Convert a word into a list of indices using the vocabulary\n",
        "    # Add <sos> token at the beginning and <eos> token at the end\n",
        "    return [vocab['<sos>']] + [vocab[ch] for ch in word] + [vocab['<eos>']]\n",
        "\n",
        "def pad_seq(seq, max_len, pad_idx=0):\n",
        "    # Pad the sequence with <pad> tokens (default index 0) to reach max_len\n",
        "    return seq + [pad_idx] * (max_len - len(seq))\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, pairs, source_vocab, target_vocab):\n",
        "        # Save padding indices for both source and target vocabularies\n",
        "        self.source_pad = source_vocab['<pad>']\n",
        "        self.target_pad = target_vocab['<pad>']\n",
        "        self.data = []\n",
        "\n",
        "        # Convert each (source, target) word pair into sequences of token indices\n",
        "        for source, target in pairs:\n",
        "            source_t = encode_word(source, source_vocab)\n",
        "            target_t = encode_word(target, target_vocab)\n",
        "            self.data.append((source_t, target_t))\n",
        "\n",
        "        # Determine the maximum lengths of source and target sequences\n",
        "        self.source_max = max(len(x[0]) for x in self.data)\n",
        "        self.target_max = max(len(x[1]) for x in self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return total number of samples in the dataset\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch a source-target pair and pad both to their respective max lengths\n",
        "        source, target = self.data[idx]\n",
        "        source = pad_seq(source, self.source_max, self.source_pad)\n",
        "        target = pad_seq(target, self.target_max, self.target_pad)\n",
        "        return torch.tensor(source), torch.tensor(target)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dimensions):\n",
        "        super().__init__()\n",
        "        # Linear layer to compute attention scores from hidden and encoder outputs\n",
        "        self.attn = nn.Linear(hid_dimensions * 2, hid_dimensions)\n",
        "\n",
        "        # Learnable vector used to reduce the attention scores to a scalar\n",
        "        self.v = nn.Parameter(torch.rand(hid_dimensions))\n",
        "\n",
        "        # Initialize vector weights uniformly\n",
        "        stdv = 1. / (hid_dimensions ** 0.5)\n",
        "        self.v.data.uniform_(-stdv, stdv)\n",
        "\n",
        "        self.hid_dimensions = hid_dimensions\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: decoder hidden state\n",
        "        # encoder_outputs: all encoder outputs for the input sequence\n",
        "\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        src_len = encoder_outputs.size(1)\n",
        "\n",
        "        # If hidden state has multiple layers, take the last one\n",
        "        if hidden.dim() == 3:\n",
        "            hidden = hidden[-1]\n",
        "        elif hidden.dim() != 2:\n",
        "            raise ValueError(f\"Expected hidden to be 2D or 3D, got shape {hidden.shape}\")\n",
        "\n",
        "        # Repeat hidden state to match the number of encoder outputs\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        # Concatenate hidden and encoder outputs, then pass through a non-linear layer\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "\n",
        "        # Compute raw attention scores using the learnable vector `v`\n",
        "        energy = energy @ self.v\n",
        "\n",
        "        # Normalize scores into a probability distribution (attention weights)\n",
        "        attn_weights = torch.softmax(energy, dim=1).unsqueeze(2)\n",
        "\n",
        "        # Compute weighted sum of encoder outputs (context vector)\n",
        "        context = torch.sum(attn_weights * encoder_outputs, dim=1)\n",
        "\n",
        "        # Return both the context vector and the attention weights\n",
        "        return context, attn_weights.squeeze(2)\n",
        "\n",
        "class translit_Decoder(nn.Module):\n",
        "    def __init__(self, output_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell='lstm'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer to convert token indices into dense vectors\n",
        "        self.embedding = nn.Embedding(output_dimensions, emb_dimensions)\n",
        "\n",
        "        # Attention module to focus on relevant parts of the encoder output\n",
        "        self.attention = Attention(hid_dimensions)\n",
        "\n",
        "        # Choose RNN type based on user-specified cell type\n",
        "        rnn_cls = {'rnn': nn.RNN, 'gru': nn.GRU, 'lstm': nn.LSTM}[cell.lower()]\n",
        "\n",
        "        # RNN layer to process embedded inputs and context\n",
        "        self.rnn = rnn_cls(\n",
        "            emb_dimensions, hid_dimensions, num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Final fully connected layer to map combined context + RNN output to vocabulary logits\n",
        "        self.fc_out = nn.Linear(hid_dimensions * 2, output_dimensions)\n",
        "\n",
        "        # Store the type of RNN cell\n",
        "        self.cell = cell.lower()\n",
        "\n",
        "        # Apply dropout to the embeddings\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        # Add time dimension to input (batch_size → batch_size x 1)\n",
        "        input = input.unsqueeze(1)\n",
        "\n",
        "        # Convert input token index to embedding and apply dropout\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        # Pass through the RNN (handle LSTM and others differently)\n",
        "        if self.cell == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        else:\n",
        "            output, hidden = self.rnn(embedded, hidden)\n",
        "            cell = None  # Non-LSTM cells don't return a separate cell state\n",
        "\n",
        "        # Use attention mechanism to compute context vector from encoder outputs\n",
        "        context, attn_weights = self.attention(hidden, encoder_outputs)\n",
        "\n",
        "        # Remove time dimension from RNN output\n",
        "        rnn_output = output.squeeze(1)\n",
        "\n",
        "        # Combine RNN output and context for final prediction\n",
        "        combined = torch.cat((rnn_output, context), dim=1)\n",
        "\n",
        "        # Compute the predicted output token scores\n",
        "        prediction = self.fc_out(combined)\n",
        "\n",
        "        # Return prediction, updated hidden/cell states, and attention weights\n",
        "        return prediction, hidden, cell, attn_weights\n",
        "\n",
        "\n",
        "class translit_Encoder(nn.Module):\n",
        "    def __init__(self, input_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell='lstm'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer to convert input indices into dense vectors\n",
        "        self.embedding = nn.Embedding(input_dimensions, emb_dimensions)\n",
        "\n",
        "        # Choose RNN type based on cell argument\n",
        "        rnn_cls = {'rnn': nn.RNN, 'gru': nn.GRU, 'lstm': nn.LSTM}[cell.lower()]\n",
        "\n",
        "        # RNN layer to process the embedded input sequence\n",
        "        self.rnn = rnn_cls(\n",
        "            emb_dimensions, hid_dimensions, num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Store attention module and cell type\n",
        "        self.attention = Attention(hid_dimensions)\n",
        "        self.cell = cell.lower()\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, source):\n",
        "        # Convert input token indices into embeddings and apply dropout\n",
        "        embedded = self.dropout(self.embedding(source))\n",
        "\n",
        "        # Pass embedded input through RNN\n",
        "        if self.cell == 'lstm':\n",
        "            outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        else:\n",
        "            outputs, hidden = self.rnn(embedded)\n",
        "            cell = None\n",
        "\n",
        "        # Compute context using attention (optional, can be ignored in basic encoder usage)\n",
        "        context = self.attention(hidden, outputs)\n",
        "\n",
        "        # Return the full sequence of encoder outputs, last hidden state, and cell state (if any)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class translit_Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder        # Encoder processes the input sequence\n",
        "        self.decoder = decoder        # Decoder generates the output sequence\n",
        "        self.device = device          # Device on which computation is performed (CPU/GPU)\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.size(0)\n",
        "        target_len = target.size(1)\n",
        "        output_dimensions = self.decoder.fc_out.out_features\n",
        "\n",
        "        # Initialize tensor to store decoder predictions for each time step\n",
        "        outputs = torch.zeros(batch_size, target_len, output_dimensions).to(self.device)\n",
        "\n",
        "        # Initialize tensor to keep track of attention weights over time\n",
        "        attn_weights_all = torch.zeros(batch_size, target_len, source.size(1)).to(self.device)\n",
        "\n",
        "        # Run the encoder on the source sequence to get hidden states\n",
        "        encoder_outputs, hidden, cell = self.encoder(source)\n",
        "\n",
        "        # Set initial decoder input to the <sos> token\n",
        "        input = target[:, 0]\n",
        "\n",
        "        # Loop over each time step in the target sequence\n",
        "        for t in range(1, target_len):\n",
        "            # Get decoder output and updated hidden states\n",
        "            output, hidden, cell, attn_weights = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "\n",
        "            # Store the current output prediction\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            # Save attention weights for this time step\n",
        "            attn_weights_all[:, t] = attn_weights\n",
        "\n",
        "            # Decide whether to use ground truth or model prediction for next input\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = target[:, t] if teacher_force else top1\n",
        "\n",
        "        # Return the full sequence of predictions and attention weights\n",
        "        return outputs, attn_weights_all\n",
        "\n",
        "\n",
        "def strip_after_eos(seq, eos_idx):\n",
        "    # Convert tensor to list if needed\n",
        "    if isinstance(seq, torch.Tensor):\n",
        "        seq = seq.cpu().numpy().tolist()\n",
        "    # Trim the sequence at the first <eos> token\n",
        "    if eos_idx in seq:\n",
        "        return seq[:seq.index(eos_idx)]\n",
        "    return seq\n",
        "\n",
        "def calculate_word_accuracy(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    correct = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        # Remove padding and stop at <eos> for fair comparison\n",
        "        pred = strip_after_eos(pred, eos_idx) if eos_idx else pred\n",
        "        target = strip_after_eos(target, eos_idx) if eos_idx else target\n",
        "        pred = [p for p in pred if p != pad_idx]\n",
        "        target = [t for t in target if t != pad_idx]\n",
        "        # Count if full predicted word matches target\n",
        "        correct += int(pred == target)\n",
        "    return correct / max(len(preds), 1)\n",
        "\n",
        "\n",
        "def calculate_cer(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    cer = 0\n",
        "    total = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        # Clean sequences by removing padding and trimming after <eos>\n",
        "        pred = strip_after_eos(pred, eos_idx) if eos_idx else pred\n",
        "        target = strip_after_eos(target, eos_idx) if eos_idx else target\n",
        "        pred = [p for p in pred if p != pad_idx]\n",
        "        target = [t for t in target if t != pad_idx]\n",
        "        # Accumulate edit distance and total characters\n",
        "        cer += editdistance.eval(pred, target)\n",
        "        total += max(len(target), 1)\n",
        "    return cer / total if total > 0 else float('inf')\n",
        "\n",
        "\n",
        "def calculate_accuracy(preds, targets, pad_idx=0, eos_idx=None):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for pred, target in zip(preds, targets):\n",
        "        # Convert tensors to lists if necessary\n",
        "        if isinstance(pred, torch.Tensor):\n",
        "            pred = pred.cpu().tolist()\n",
        "        if isinstance(target, torch.Tensor):\n",
        "            target = target.cpu().tolist()\n",
        "        # Strip <eos> tokens if specified\n",
        "        if eos_idx is not None:\n",
        "            pred = strip_after_eos(pred, eos_idx)\n",
        "            target = strip_after_eos(target, eos_idx)\n",
        "        # Compare tokens one by one, ignoring padding\n",
        "        for p_token, t_token in zip(pred, target):\n",
        "            if t_token == pad_idx:\n",
        "                continue\n",
        "            if p_token == t_token:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return correct / total if total > 0 else 0.0"
      ],
      "metadata": {
        "id": "eqP_qshcWqfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "3g9F1-ANrbbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "\n",
        "# Verify font file\n",
        "!ls /content\n",
        "\n",
        "# Font path\n",
        "font_path = '/content/NotoSansTelugu-VariableFont.ttf'\n",
        "import os\n",
        "if os.path.exists(font_path):\n",
        "    print(f\"Font file found at {font_path}\")\n",
        "else:\n",
        "    print(f\"Font file not found at {font_path}. Please upload it.\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()"
      ],
      "metadata": {
        "id": "tyFSj1ergZ6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "def create_heatmap_image(src_tokens, pred_tokens, attn_weights, idx, idx2char_src, idx2char_tgt):\n",
        "    \"\"\"Create a single heatmap image for WandB table using PIL\"\"\"\n",
        "    # Define image dimensions\n",
        "    cell_size = 50\n",
        "    label_width = 150\n",
        "    margin = 50\n",
        "    title_height = 50\n",
        "    xlabel_height = 50\n",
        "    ylabel_width = 100\n",
        "\n",
        "    # Filter tokens and get labels\n",
        "    src_labels = [idx2char_src.get(idx, '?') for idx in src_tokens if idx in idx2char_src and idx not in [source_vocab.get('<pad>', -1), source_vocab.get('<sos>', -1), source_vocab.get('<eos>', -1)]]\n",
        "    pred_labels = [idx2char_tgt.get(idx, '?') for idx in pred_tokens if idx in idx2char_tgt and idx not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1), target_vocab.get('<eos>', -1)]]\n",
        "\n",
        "    # Debug labels with Unicode code points\n",
        "    print(f\"Example {idx+1} - Source tokens: {src_tokens}\")\n",
        "    print(f\"Example {idx+1} - Source labels: {src_labels}\")\n",
        "    print(f\"Example {idx+1} - Pred tokens: {pred_tokens}\")\n",
        "    print(f\"Example {idx+1} - Pred labels: {pred_labels}\")\n",
        "    print(f\"Example {idx+1} - Pred labels (Unicode): {[f'U+{ord(c):04X}' for c in pred_labels if c != '?']}\")\n",
        "    print(f\"Example {idx+1} - Attention weights shape: {attn_weights.shape}\")\n",
        "\n",
        "    # Filter out invalid characters\n",
        "    valid_pred_labels = []\n",
        "    for char in pred_labels:\n",
        "        if char == '?' or not (0x0C00 <= ord(char) <= 0x0C7F):  # Telugu Unicode range\n",
        "            valid_pred_labels.append('?')  # Replace invalid chars with '?'\n",
        "        else:\n",
        "            valid_pred_labels.append(char)\n",
        "    pred_labels = valid_pred_labels\n",
        "    print(f\"Example {idx+1} - Filtered pred_labels: {pred_labels}\")\n",
        "\n",
        "    # Truncate attention weights to match label lengths\n",
        "    attn_weights = attn_weights[:min(len(pred_labels), attn_weights.shape[0]),\n",
        "                                :min(len(src_labels), attn_weights.shape[1])]\n",
        "    print(f\"Example {idx+1} - Truncated attention weights shape: {attn_weights.shape}\")\n",
        "\n",
        "    # Calculate image size\n",
        "    heatmap_width = len(src_labels) * cell_size\n",
        "    heatmap_height = len(pred_labels) * cell_size\n",
        "    img_width = heatmap_width + label_width + ylabel_width + 2 * margin\n",
        "    img_height = heatmap_height + label_width + title_height + xlabel_height + 2 * margin\n",
        "\n",
        "    # Create a new image with white background\n",
        "    image = Image.new('RGB', (img_width, img_height), 'white')\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Load Telugu font\n",
        "    font_path = '/content/LohitTeluguRegular.ttf'\n",
        "    if not os.path.exists(font_path):\n",
        "        raise FileNotFoundError(f\"Telugu font not found at {font_path}.\")\n",
        "    try:\n",
        "        telugu_font = ImageFont.truetype(font_path, size=20)\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to load font {font_path}: {e}\")\n",
        "\n",
        "    # Default font for Latin text (use FreeSans if available, or fallback to default)\n",
        "    try:\n",
        "        latin_font = ImageFont.truetype('/usr/share/fonts/truetype/freefont/FreeSans.ttf', size=20)\n",
        "    except:\n",
        "        latin_font = ImageFont.load_default()\n",
        "\n",
        "    # Draw title\n",
        "    title = f'Example {idx+1}'\n",
        "    draw.text((margin + ylabel_width, margin), title, font=latin_font, fill='black')\n",
        "\n",
        "    # Draw x-axis labels (Source Tokens - Latin)\n",
        "    for i, label in enumerate(src_labels):\n",
        "        x = margin + ylabel_width + i * cell_size + cell_size // 2\n",
        "        y = margin + title_height + heatmap_height + 10\n",
        "        draw.text((x, y), label, font=latin_font, fill='black', anchor='mm')\n",
        "\n",
        "    # Draw y-axis labels (Target Tokens - Telugu)\n",
        "    for i, label in enumerate(pred_labels):\n",
        "        x = margin + ylabel_width - 10\n",
        "        y = margin + title_height + i * cell_size + cell_size // 2\n",
        "        draw.text((x, y), label, font=telugu_font, fill='black', anchor='rm')\n",
        "\n",
        "    # Draw x-axis title\n",
        "    draw.text((margin + ylabel_width + heatmap_width // 2, margin + title_height + heatmap_height + xlabel_height - 10),\n",
        "              'Source Tokens (Latin)', font=latin_font, fill='black', anchor='mm')\n",
        "\n",
        "    # Draw y-axis title\n",
        "    draw.text((margin + ylabel_width // 2, margin + title_height + heatmap_height // 2),\n",
        "              'Target Tokens (Telugu)', font=telugu_font, fill='black', angle=90, anchor='mm')\n",
        "\n",
        "    # Draw heatmap\n",
        "    for i in range(len(pred_labels)):\n",
        "        for j in range(len(src_labels)):\n",
        "            # Normalize attention weights to [0, 1] for color mapping\n",
        "            weight = attn_weights[i, j]\n",
        "            # Map to a color (viridis-like: 0=blue, 1=yellow)\n",
        "            r = int(255 * weight)\n",
        "            g = int(255 * (1 - weight))\n",
        "            b = 0\n",
        "            color = (r, g, b)\n",
        "            x0 = margin + ylabel_width + j * cell_size\n",
        "            y0 = margin + title_height + i * cell_size\n",
        "            draw.rectangle([x0, y0, x0 + cell_size, y0 + cell_size], fill=color)\n",
        "\n",
        "    # Save image for verification\n",
        "    if idx == 0:\n",
        "        image.save(f\"/content/heatmap_example_{idx+1}.png\")\n",
        "        print(f\"Saved sample heatmap to /content/heatmap_example_{idx+1}.png\")\n",
        "\n",
        "    # Convert to WandB image\n",
        "    wandb_image = wandb.Image(image, caption=f\"Attention Heatmap Example {idx+1}\")\n",
        "    return wandb_image"
      ],
      "metadata": {
        "id": "M9sqCVD8lDi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "run = wandb.init(project=\"dakshina-seq2seq\", entity=\"sai-sakunthala-indian-institute-of-technology-madras\", name=\"evaluate_test\")\n",
        "artifact = run.use_artifact('sai-sakunthala-indian-institute-of-technology-madras/dakshina-seq2seq-3/best_model:v52', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "# Read data and create vocabularies\n",
        "test_pairs = read_data(data_path + f\"{LANG}.translit.sampled.test.tsv\", max_len=30)\n",
        "train_pairs = read_data(data_path + f\"{LANG}.translit.sampled.train.tsv\", max_len=30)\n",
        "source_vocab, idx2char_src = make_vocab([x[0] for x in train_pairs])\n",
        "target_vocab, idx2char_tgt = make_vocab([x[1] for x in train_pairs])\n",
        "\n",
        "# Model parameters (must match training)\n",
        "input_dimensions = len(source_vocab)\n",
        "output_dimensions = len(target_vocab)\n",
        "emb_dimensions = 128\n",
        "hid_dimensions = 128 * 2\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "cell = 'lstm'\n",
        "batch_size = 64\n",
        "max_len = 30\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize model\n",
        "encoder = translit_Encoder(input_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell).to(device)\n",
        "decoder = translit_Decoder(output_dimensions, emb_dimensions, hid_dimensions, num_layers, dropout, cell).to(device)\n",
        "model = translit_Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "# Load model weights\n",
        "state_dict = torch.load(f\"{artifact_dir}/best_model.pt\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Create test dataset and loader\n",
        "test_translit = TransliterationDataset(test_pairs, source_vocab, target_vocab)\n",
        "test_loader = DataLoader(test_translit, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "all_src, all_preds, all_tgts, all_attn_weights = [], [], [], []\n",
        "correct = 0\n",
        "total = 0\n",
        "selected_examples = []\n",
        "\n",
        "def predict(model, src, max_len=30):\n",
        "    \"\"\"Greedy decoding implementation with attention weights\"\"\"\n",
        "    encoder_outputs, encoder_hidden, encoder_cell = model.encoder(src)\n",
        "    input = torch.tensor([target_vocab['<sos>']] * src.size(0)).to(device)\n",
        "    outputs = []\n",
        "    attn_weights_list = []\n",
        "\n",
        "    for t in range(max_len):\n",
        "        output, encoder_hidden, encoder_cell, attn_weights = model.decoder(input, encoder_hidden, encoder_cell, encoder_outputs)\n",
        "        input = output.argmax(1)\n",
        "        outputs.append(input)\n",
        "        attn_weights_list.append(attn_weights)\n",
        "\n",
        "        if (input == target_vocab.get('<eos>', -1)).all():\n",
        "            break\n",
        "\n",
        "    outputs = torch.stack(outputs, dim=1)  # (batch_size, max_len)\n",
        "    attn_weights_all = torch.stack(attn_weights_list, dim=1)  # (batch_size, max_len, src_len)\n",
        "    return outputs, attn_weights_all\n",
        "\n",
        "def log_attention_heatmaps_individually(sources, predictions, attn_weights_list, idx2char_src, idx2char_tgt, num_plots=10):\n",
        "    for i in range(min(num_plots, len(sources))):\n",
        "        heatmap_img = create_heatmap_image(\n",
        "            sources[i], predictions[i], attn_weights_list[i], i, idx2char_src, idx2char_tgt\n",
        "        )\n",
        "        wandb.log({f\"Attention Heatmap {i+1}\": wandb.Image(heatmap_img)})\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, tgt in tqdm(test_loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        preds, attn_weights_batch = predict(model, src)\n",
        "\n",
        "        # Convert to numpy arrays for processing\n",
        "        src_np = src.cpu().numpy()\n",
        "        preds_np = preds.cpu().numpy()\n",
        "        tgt_np = tgt.cpu().numpy()\n",
        "        attn_weights_np = attn_weights_batch.cpu().numpy()  # (batch_size, max_len, src_len)\n",
        "\n",
        "        for i in range(len(src_np)):\n",
        "            # Get source, prediction, target, and attention weights\n",
        "            s = src_np[i]\n",
        "            p = preds_np[i]\n",
        "            t = tgt_np[i]\n",
        "            attn = attn_weights_np[i]\n",
        "\n",
        "            # Store sequences and attention weights for all examples\n",
        "            all_src.append(s)\n",
        "            all_preds.append(p)\n",
        "            all_tgts.append(t)\n",
        "            all_attn_weights.append(attn)\n",
        "\n",
        "            # Collect up to 12 examples for heatmaps\n",
        "            if len(selected_examples) < 12:\n",
        "                selected_examples.append((s, p, t, attn))\n",
        "\n",
        "            # Process prediction: remove padding and everything after EOS\n",
        "            p_processed = []\n",
        "            for token in p:\n",
        "                if token == target_vocab.get('<eos>', -1):\n",
        "                    break\n",
        "                if token not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1)]:\n",
        "                    p_processed.append(token)\n",
        "\n",
        "            # Process target: remove padding and everything after EOS\n",
        "            t_processed = []\n",
        "            for token in t:\n",
        "                if token == target_vocab.get('<eos>', -1):\n",
        "                    break\n",
        "                if token not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1)]:\n",
        "                    t_processed.append(token)\n",
        "\n",
        "            # Compare the processed sequences\n",
        "            if p_processed == t_processed:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "# Plot and log attention heatmaps for 12 examples\n",
        "if len(selected_examples) >= 12:\n",
        "    sources, predictions, _, attn_weights_list = zip(*selected_examples[:12])\n",
        "    log_attention_heatmaps_individually(sources, predictions, attn_weights_list, idx2char_src, idx2char_tgt, num_plots=12)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct / total if total > 0 else 0\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Correct: {correct}, Total: {total}\")\n",
        "wandb.log({\"Test Accuracy\": accuracy})\n",
        "\n",
        "\n",
        "def log_table_wandb(sources, preds, targets, idx2char_src, idx2char_tgt, num_samples=10, min_correct=7):\n",
        "\n",
        "    # Create W&B table with relevant column headers\n",
        "    table = wandb.Table(columns=[\"Source\", \"Prediction\", \"Reference\", \"Status\"])\n",
        "\n",
        "    total_samples = len(sources)\n",
        "    indices = random.sample(range(total_samples), min(num_samples, total_samples))\n",
        "\n",
        "    for i in indices:\n",
        "        # Convert index sequences to readable strings, skipping special tokens\n",
        "        src_word = ''.join([idx2char_src.get(idx, '?') for idx in sources[i]\n",
        "                            if idx not in {source_vocab.get('<pad>'), source_vocab.get('<sos>'), source_vocab.get('<eos>')}])\n",
        "\n",
        "        pred_word = ''.join([idx2char_tgt.get(idx, '?') for idx in preds[i]\n",
        "                             if idx not in {target_vocab.get('<pad>'), target_vocab.get('<sos>'), target_vocab.get('<eos>')}])\n",
        "\n",
        "        ref_word = ''.join([idx2char_tgt.get(idx, '?') for idx in targets[i]\n",
        "                            if idx not in {target_vocab.get('<pad>'), target_vocab.get('<sos>'), target_vocab.get('<eos>')}])\n",
        "\n",
        "        # Determine if prediction matches the reference\n",
        "        is_correct = pred_word == ref_word\n",
        "        status = \"🟩 **Correct**\" if is_correct else \"🟥 **Incorrect**\"\n",
        "\n",
        "        # Add row to table\n",
        "        table.add_data(src_word, pred_word, ref_word, status)\n",
        "\n",
        "    # Log the table to Weights & Biases\n",
        "    wandb.log({\"Test Sample Predictions (Random)\": table})\n",
        "    print(f\"Logged {len(indices)} random predictions to W&B\")\n",
        "\n",
        "#log table\n",
        "log_table_wandb(all_src, all_preds, all_tgts, idx2char_src, idx2char_tgt, num_samples=10, min_correct=7)\n",
        "\n",
        "# Save predictions to file\n",
        "output_dir = \"predictions_attention\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(output_dir, \"test_predictions.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for s, p, t in zip(all_src, all_preds, all_tgts):\n",
        "        src_word = ''.join([idx2char_src.get(idx, '?') for idx in s if idx not in [source_vocab.get('<pad>', -1), source_vocab.get('<sos>', -1), source_vocab.get('<eos>', -1)]])\n",
        "        pred_word = ''.join([idx2char_tgt.get(idx, '?') for idx in p if idx not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1), target_vocab.get('<eos>', -1)]])\n",
        "        ref_word = ''.join([idx2char_tgt.get(idx, '?') for idx in t if idx not in [target_vocab.get('<pad>', -1), target_vocab.get('<sos>', -1), target_vocab.get('<eos>', -1)]])\n",
        "        f.write(f\"{src_word}\\t{pred_word}\\t{ref_word}\\n\")\n",
        "\n",
        "print(f\"Saved full predictions to: {output_dir}/test_predictions.txt\")\n",
        "wandb.save(os.path.join(output_dir, \"test_predictions.txt\"))\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "LJ4TX8mGsMzu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}